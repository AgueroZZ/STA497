---
title: "Posterior Approximation for Case-Crossover Models with Structured Additive Predictors"
author: "Alex Stringer"
date: "2019-04-05"
output: 
  pdf_document:
    keep_tex: true
    number_sections: true
    fig_caption: yes
header_includes:
  - \newcommand{\ed}{\overset{d}{=}}
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(cowplot)
library(survival)
library(INLA)
library(Matrix)
library(ipoptr)
library(brinla)
```

# Summary and state of project {-}

# Introduction

## Motivation and proposed work

When studying the association of mortality with short-term exposures to risk factors, for example air pollution or extreme temperature, retrospectively-sampled observational data are common. It would usually be unethical to perform a randomized experiment in such a setting and performing a prospective sample would under-sample the case group, as deaths attributable to exposure to any single risk factor are often rare. The most practical method of data collection is therefore to sample from those known to have died due to exposure during some fixed time period and compare their exposure at time of death to exposure at one or more “control times”, when they did not die. This is an especially attractive sampling scheme if the risk factors under consideration are the types of things that are already being recorded as part of large administrative databases available to the researcher, like weather or air pollution data.

The case-crossover design allows for modelling the effect of exposure on mortality for data sampled in this way. In such a design, each subject's contribution to the likelihood consists of the probability of them dying when they did, and not dying at any of the control times considered. This gives the associated probability model an explicit connection to stratified Cox Proportional Hazards regression. Current methods of fitting case-crossover models make use of this equivalence and as a result, there is no single, dedicated, documented software package for fitting these models with linear and smooth terms, and random effects. Since these models are being used in medical and epidemiological research, a single source of methodology and application of these models represents an overdue contribution to the literature.

In this paper, we will consider the case-crossover model in a Bayesian context, and develop an approximation-based method of inference for the posterior distribution of the exposure effects. This will allow a single comprehensive interface for fitting case-crossover models with any and all of linear and smooth terms and random effects, with multiple different covariance structures allowing for structured dependence between observations; time series models, and random-walk smoothing; and principled uncertainty estimates in all cases. A well-documented software package will accompany this research, the result being a one-stop shop for practitioners who want to fit case-crossover models to their data.

## Examples 

@casecrossover introduces the case-crossover design as a method for studying the hypothesis that myocardial infarctions (essentially heart attacks) do not occur at random for individuals, but are influenced by short-term exposure to risk factors in the time immediately before the event. For example, are short-term bursts of anger, physical exertion, or over-eating associated with a positive increase in risk of heart attacks? The key point here is that while the risk of \textit{long-term} exposure to these risk factors is well-established, the specific motivation for this design is to determine the association of the event of interest with \textit{short-term} exposure. This allows the practitioner to assess whether, all else equal between them, an individual who over-eats or experiences a bout of intense anger has a \textit{temporarily} increased risk of experiencing a heart attack over an individual who does not experience these events.

@cellphone utilized the case-crossover design for determining the relative risk of drivers having a serious accident while using a cell phone compared to while not. They sampled their data by interviewing subjects at motor vehicle collision centres in the Greater Toronto Area, and comparing their cell phone use (``exposure'') in the time leading up to their accidents to their cell phone use on a previous day when they undertook a similar amount of driving, but when they did not experience an accident. Compare this to how else these data could have been obtained---by asking some people to talk on the phone while driving and some to not and seeing who crashed (randomized experiment; highly unethical), or by randomly sampling individuals and asking if they crashed on two chosen days and comparing their cell phone usage (prospective; will result in very sparse data on crashes). The case-crossover design provides a method for answering this research question using a relatively cost-effective and ethical method of data collection.

More recently, @temperature consider the effect of short-term exposure to both extreme and moderate temperatures on mortality in India. Using a large, nationally-representitive dataset that was not collected for this specific purpose, the authors were able to associate short term exposure to extremely hot, but also moderately cold, temperatures to mortality. This illustrates an advantage of the case-crossover design; modern data is often collected on a large scale with no \textit{specific} purpose with the expectation that researchers will anlayse it in support of numerous different goals. Whereas in the previous examples, there was choice about how to collect the data, with the type of sampling amenable to analysis using the case-crossover design being the most feasible, here there is no choice, as the data is already collected. The case-crossover design here enabled the authors to associate mortality with exposure to moderately cold temperatures, a novel finding with mitigation policy implications.

## Existing methods of fitting

There are several methods available in the `R` language for fitting case-crossover models. To reiterate, the purpose of this work is to provide a \textit{unified, principled}, and \textit{documented} framework for fitting these models with a combination of linear and smooth terms, and different random effects structures. To provide context, here we illustrate how case-crossover methods may currently be fit to data, and the drawbacks of each. One global drawback that can't be overstated is that none of the following approaches are well documented; with the exception of `survival::clogit`, they all exist as examples buried away in documentation for other functions or online support lists. All make use of the equivalent functional forms of the likelihoods for the case-crossover model and Cox's stratified proportional hazards regression.

In all of the below one-line code snippets, it is assumed the analyst is working with a dataframe `dat` containing multiple rows per subject with subject identifier `id`; an indicator `case` which equals $1$ for exactly one row per subject and $0$ else, and covariates `x1` and `x2`.

### `survival::clogit`

The `survival` package provides a `clogit` function for fitting case-crossover (or more generally, conditional logistic) models. All this function does is prepare the data in an appropriate way and call `survival::coxph`. The drawbacks are
\begin{itemize}
\item \textbf{Restricted model class}: only linear terms are permitted.
\item \textbf{Documentation} There is limited documentation, and no fully-worked examples.
\end{itemize}
To fit the model with linear terms, the analyst would run
```{r clogit-1,eval=FALSE}
clogit(case ~ x1 + x2 + strata(id),data = dat)
```
and receive back a `coxph` object with the usual available methods.

### `mgcv::gam`

The `gam` function in the `mgcv` package, used for fitting generalized additive models, may be used to fit case-crossover models with smooth terms for the covariates, using the same trick as `survival::clogit`. Some drawbacks are:
\begin{itemize}
\item \textbf{Ridgidness of smoothing methodology}. There are a lot of options available within \texttt{mgcv::gam}, but each one requires separate knowledge and has parameters and options that need to be tuned. The resulting interface is intimidating and requires expert knowledge to apply. While it is easy to switch between different smoothing methods in the software interface, a non-expert user will lack the ability to make a principled choice between them.
\item \textbf{Computational efficiency}. Running \texttt{mgcv::gam} with three smooth terms on a dataset with approximately $40,000$ rows takes around $11$ minutes on a modern laptop. This is not prohibitive, but it's not convenient, and such a dataset and model are not at all unrepresentitive of the scale at which these models are fit in real world applications.
\item \textbf{Documentation}. The only available documentation for fitting case-crossover methods in this way is found at the end of the help page for the \texttt{cox.ph} family, \texttt{?mgcv::cox.ph}. The \texttt{mgcv::gam} function itself is documented excellently, but in order to make use of this, the user has to have detailed knowledge of GAMs and the associated theory. In the common case where GAMs are being investigated because a model with only linear terms reults in unsatisfactory fit, this might not be the case, and might serve as a deterrent.
\end{itemize}
To fit a model with smooth terms, the analyst would run the somewhat confusing
```{r mgcv-1,eval=FALSE}
mgcv::gam(
    cbind(rep(1,nrow(dat)),dat$id) ~ s(x1) + s(x2),
    data = dat,
    family = mgcv::cox.ph,
    weights = case
  )
```
and receive back an object of class `gam` with the usual available methods.

### `R-INLA`

To fit the case-crossover models in a Bayesian framework with a structured additive predictor, the analyst can make use of the `R-INLA` software. INLA is described in detail in sections 2 and 3 as it will be the basis of our approximations. The software allows for fitting of linear and smooth terms and random effects, with a variety of structures. The drawbacks of this approach are:
\begin{itemize}
\item \textbf{Computationally intensive}. The speed and memory requirements to use INLA for Cox-PH models in general are prohibitive when there are many strata.
\item \textbf{Documentation}. The INLA package is documented largely through an FAQ and extensive online support group containing user questions and developer responses. This makes using INLA at all a daunting task for a practitioner; to use it for a "trick" model like this is nearly impossible to figure out for someone without expert knowledge of both INLA and case-crossover designs.
\item \textbf{Incompatability}. In raw form, the Cox Proportional Hazards regression model is not compatable with the INLA framework, because the likelihood for each death time depends on more than one element of the underlying "latent field". INLA uses tricks to get around this, which brings us back to the documentation drawback.
\end{itemize}
To fit a model with linear terms, the analyst would run the \textit{very} confusing
```{r inla-1, eval=FALSE}
survobj <- inla.surv(
  time = rep(1,nrow(dat)),
  event = dat$case
)

system.time({
  inla_clogit_mod <- inla(
    survobj ~ x1 + x2
    data = dat,
    family = "coxph",
    verbose = TRUE,
    control.hazard=list(model="rw1", n.intervals=2,strata.name = "id")
  )
})
```

All of INLA's flexibility could be used here to include models with smooth terms and random effects with different structures, \textit{in theory}. In practice, when run on a dataset with around $40,000$ observations and the above three linear terms only, the model crashes; on a cloud virtual machine with $8$ cores and $64$ GB of RAM, there is not enough memory. The problem is that INLA uses a smoothing model to estimate the hazard function, which works well when there are no strata but fails when there are many strata (and hence many hazard functions) with a small number of observations in each. It is documented in the INLA support groups online that there is no way to turn this off, and that it is not a priority for future releases.


## Outline of paper

The discussion in section 1.3 suggests that INLA would be the ideal framework in which to set our general treatment of case-crossover models with structured additive predictors. In section 2, we will develop background theory on case-crossover models and INLA. In section 3, we develop the approximation to the posterior distribution of the latent variables that we will use to fit the case-crossover model to data. Section 4 presents worked examples and section 5 concludes with a discussion.

# Methodology

## Case-Crossover model

In this paper, we parametrize the case-crossover model as a structured additive model. In this section, we explain the details of this parametrization and derive the likelihood for the model.

### Structured additive models: the linear predictor

The case-crossover model is parametrized as a \textbf{structured additive model}. We start with a linear predictor:
\begin{equation}\label{eqn:linearpredictor}
\eta_{it} = \alpha_{i}(t) + \sum_{q=1}^{M_{1}}\beta_{q}z_{qi}(t) + \sum_{q=1}^{M_{2}}f_{q}(u_{qi}(t))
\end{equation}
This is really a \textit{structured additive predictor}; we use the term "linear predictor" throughout to improve readability. This is the most general form allowable, containing unstructured smooth, structured smooth, and linear terms. Our notation is as follows:
\begin{enumerate}
\item Indices:
  \begin{enumerate}
  \item $i = 1,\ldots,n$ for subjects,
  \item $t = 1,\ldots,T_{i}$ for time points.
  \end{enumerate}
\item Each subject is observed at times $t \in S_{i} \equiv \left\{ 1,\ldots,T_{i}\right\}$, referred to as the \textbf{referent frame}. A key feature of the case-crossover study is that $S_{i}$ is \textit{chosen by the experimenter} for each subject,
\item $\alpha_{i}(t)$ represents the subject-specific mortality propensity at time $t$. This is an unstructured random effect included to capture the haphazard variation in subjects' individual mortality propensities. It will be demonstrated that the experimenter must choose $S_{i}$ for each subject such that these terms cancel out of the likelihood,
\item $z_{qi}(t)$ is the value of the $q^{th}$ covariate, representing exposures to risk factors, $q = 1,\ldots,M_{1}$, on the $i^{th}$ subject at the $t^{th}$ observation time, which we choose to model \textbf{linearly} with cofficients $\beta_{q}$,
\item $u_{qi}(t)$ is the value of the $q^{th}$ covariate, $q = 1,\ldots,M_{2}$, which we choose to model via \textbf{arbitrary smooth functions} $f_{q}$. These represent structured random effects, included to capture the structural changes in mortality propensity associated with exposure to these risk factors that are not well modelled by linear terms. In the Bayesian framework, we don't need to specify what they are, rather we put a prior on the joint distributions of their \textit{realizations} at the observed covariates.
\begin{itemize}
\item \textit{Note:} the $f_{q}$ terms may also be taken to be classical random effects; in the Bayesian structured additive model framework, no mathematical distinction is made between these and smooth terms.
\end{itemize}
\end{enumerate}
The choice of referent frame $S_{i}$ is highly important in order to identify the effects in the model. Specifically, the control days must be chosen so that they resemble the death day in all non exposure-related mortality risks, allowing us to consider $\alpha_{i}(t)$ to be approximately constant with respect to $t$ for each subject. This will allow $\alpha_{i} \equiv \alpha_{i}(t)$ to cancel out of likelihood calculations, and corresponds to each subject "serving as their own control". This isn't as esoteric or unachievable as it sounds; examples of how this is done can be found in the three studies discussed in section 1. For example, for @cellphone, "exposure" was "using a cell phone while driving", so the authors chose control days where the subjects drove a similar amount and for similar reasons as on their crash days. It wouldn't make sense to compare a day of heavy commuting into the city with a day of light errand-running in the suburbs in terms of overall crash risk, regardless of cell phone use. Other potential factors in this context could include the weather (snowy days may have more crashes) and so on.

### The probability models

With a probability model linking the linear predictor to the mean response, inference is based on a partial likelihood formed by conditioning on the fact that one and only one death occurred for each subject. Because of this, there are two probability models that lead to identical inferences. The first is a binary logistic model. Denote by $Y_{it}$ the indicator of whether subject $i$ died at time $t$. With the linear predictor $\eta_{it}$ defined in (\ref{eqn:linearpredictor}), our probability model is
\begin{equation}\label{eqn:bernoullimodel}
\begin{aligned}
Y_{it} &\sim \text{Bernoulli}(p_{it}) \\
\log\left(\frac{p_{it}}{1-p_{it}}\right) &= \eta_{it}
\end{aligned}
\end{equation}
We may also consider a Poisson model, where we model the count of deaths per subject directly:
\begin{equation}\label{eqn:poissonmodel}
\begin{aligned}
Y_{it} &\sim \text{Poisson}(\lambda_{it}) \\
\log\lambda_{it} &= \eta_{it}
\end{aligned}
\end{equation}
In what follows, these two models will be seen to lead to the same partial likelihood for $\eta_{it}$, when conditioning is done in the appropriate way. The informal intuition for this is as follows. When conditioning on one and only one event occurring per subject, the binary logistic model is really a multinomial model, where subjects' contribution to the likelihood consists of the probability of them dying when they did, and \textit{not} dying at any time before that. Hence the vector of indicators $Y_{i} = (Y_{i0},\ldots,Y_{iT_{i}})$ is modelled as $\text{Multinomial}(1,p_{i0},\ldots,p_{iT_{i}})$ and this is used to form the likelihood. Under the Poisson model, the same conditioning amounts to modelling the joint distribution of $(Y_{i0},\ldots,Y_{iT_{i}})|\sum_{t\in S_{i}} Y_{it} = 1$, which is known to be multinomial with parameters $(\lambda_{i0},\ldots,\lambda_{iT_{i}}) / \sum_{t\in S_{i}}\lambda_{it}$. The result is (\ref{eqn:bernoullimodel}) and (\ref{eqn:poissonmodel}) leading to the same inferences.

We now derive the conditional likelihood for the case crossover model. To provide context and understanding, the derivation is presented in detail for the Bernoulli model with a single control day and the math is then extended to multiple control days.

### Binary logistic model: single control day

With a single control day, subjects' exposures are measured once at death, and then at one additional pre-death time. This makes these models nearly the same as the Matched Pairs models described by @coxabd. Without loss of generality, let $t = 0$ represent the control day and $t = 1$ the case day (when they died) for each subject. Assume as well that the experimenter has chosen the control day to be "like" the case-day in terms of other unmeasured mortality risk, so that we may take $\alpha_{i}(t) \equiv \alpha_{i}$. We start with:

\begin{equation}\label{eqn:ccmmodel1}
\begin{aligned}
p_{it} &= P\left(Y_{i} = t | z\right) \\
\log\left( \frac{p_{it}}{1 - p_{it}} \right)  &= \alpha_{i} + \eta_{it} \\
\implies p_{it} &= \frac{\exp(\alpha_{i} + \eta_{it})}{1 + \exp(\alpha_{i} + \eta_{it})} \\
\implies 1 - p_{it} &= \frac{1}{1 + \exp(\alpha_{i} + \eta_{it})} \\
\end{aligned}
\end{equation}

With $\alpha_{i} \equiv \alpha_{i}(t)$ taken to be constant in time, $\eta_{it}$ represents the additional mortality risk over a subject's baseline when exposure is at $(z_{i}(t), u_{i}(t))$. @coxabd starts with something similar, taking the control observation to have logit-probability equal to $\alpha_{i}$ and the case observation having logit-probability equal to $\alpha_{i} + \Delta$, with $\Delta$ representing the treatment effect. This results in probability modelled as
\begin{equation}
p_{i1} = \frac{e^{\Delta}}{1 + e^{\Delta}}
\end{equation}

In the derivation here, $\Delta$ corresponds to $\eta_{i1} - \eta_{i0}$, the difference in exposure risks at case vs control times. If $\eta_{it}$ contained only linear terms, our construction here would be equivalent to performing a logistic regression on the differences in exposure on the case vs control days. Fundamental to our approach is its ability to handle smooth terms and linear terms simultaneously, which cannot be achieved by simply differencing the data prior to analysis.

The $i^{th}$ subject's contribution to the likelihood, $L_{i}$, is the probability that they die when they did, given that they died exactly once. This is enforced by the retrospective sampling and is compactly written as $\left\{Y_{i0} = 0,Y_{i1} = 1 | Y_{i0} + Y_{i1} = 1\right\}$. To get the likelihood, first note that the left-hand event is nested within the right-hand event,
\begin{equation}
\left\{ Y_{i0} = 0, Y_{i1} = 1 \right\} \subset \left\{ Y_{i0} + Y_{i1} = 1\right\}
\end{equation}
so that the joint probability of these two events equals the marginal probability of the former. It then follows from the rules of conditional probability that
\begin{equation}\label{eqn:lik2group}
\begin{aligned}
L_{i} &= P\left( Y_{i0} = 0, Y_{i1} = 1 \big\vert Y_{i0} + Y_{i1} = 1\right) \\
&= \frac{
P\left( Y_{i0} = 0, Y_{i1} = 1 , Y_{i0} + Y_{i1} = 1\right)
}{
P\left(Y_{i0} + Y_{i1} = 1\right)
} \\
&= \frac{
\left( \frac{\exp(\alpha_{i} + \eta_{i1})}{1 + \exp(\alpha_{i} + \eta_{i1})}\right) 
\times \left( \frac{1}{1 + \exp(\alpha_{i} + \eta_{i0})}\right)
}{
\left( \frac{\exp(\alpha_{i} + \eta_{i1})}{1 + \exp(\alpha_{i} + \eta_{i1})}\right) 
\times \left( \frac{1}{1 + \exp(\alpha_{i} + \eta_{i0})}\right) +
\left( \frac{\exp(\alpha_{i} + \eta_{i0})}{1 + \exp(\alpha_{i} + \eta_{i0})}\right) 
\times \left( \frac{1}{1 + \exp(\alpha_{i} + \eta_{i1})}\right)
} \\
&= \frac{
\exp(\alpha_{i} + \eta_{i1})
}{
\exp(\alpha_{i} + \eta_{i0}) + \exp(\alpha_{i} + \eta_{i1})
} \\
&= \frac{
\exp(\eta_{i1})
}{
\exp(\eta_{i0}) + \exp(\eta_{i1})
} 
\end{aligned}
\end{equation}

The subject-specific mortality risk $\alpha_{i}$ cancels, because subjects serve as their own controls. The form shown in (\ref{eqn:lik2group}) is the form that the likelihood is commonly presented in; multiplying through by $\exp(-\eta_{i0})$ on the top and bottom explicitly exposes the connection to Cox's $\Delta$:

\begin{equation}
\begin{aligned}
\frac{
\exp(\eta_{i1})
}{
\exp(\eta_{i0}) + \exp(\eta_{i1})
} 
&= \frac{
\exp(\eta_{i1} - \eta_{i0})
}{
1 + \exp(\eta_{i1} - \eta_{i0})
} \\
&= \frac{e^{\Delta}}{1+e^{\Delta}}
\end{aligned}
\end{equation}
The interested reader may verify that the same conditional likelihood is obtained by starting with a Poisson model in which $\log\mathbb{E}(Y_{it}) = \eta_{it}$.

### Multiple control days

The derivation for multiple control days is the same but less illuminating, simply due to the messier notation. Define for each subject the \textit{referent scheme} $S_{i} = \left\{ 0,1,\ldots,T_{i}\right\}$ as the labelling of all the days on which subject $i$ was observed, with $T_{i}$ indicating the death time (again, without loss of generality---these are just labels). The probability model and likelihood are constructed in exactly the same way:
\begin{equation}
\begin{aligned}
L_{i} &= P\left( Y_{i0} = 0,Y_{i1} = 0,\ldots, Y_{iT_{i}} = 1 \big\vert Y_{i0} + Y_{i1} + \cdots + Y_{iT_{i}} = 1\right) \\
&= \frac{
P\left( Y_{i0} = 0,Y_{i1} = 0,\ldots, Y_{iT_{i}} = 1 , Y_{i0} + Y_{i1} + \cdots + Y_{iT_{i}} = 1\right)
}{
P\left(Y_{i0} + Y_{i1} + \cdots + Y_{iT_{i}} = 1\right)
} \\
&= \frac{
\frac{\exp(\alpha_{i} + \eta_{iT_{i}})}{1 + \exp(\alpha_{i} + \eta{iT_{i}})}\times \prod_{t=0}^{T_{i}-1}\frac{1}{1 + \exp(\alpha_{i} + \eta_{it})}
}{
\sum_{t=0}^{T_{i}}
\left(
\frac{
\exp(\alpha_{i} + \eta_{it})
}{
1 + \exp(\alpha_{i} + \eta_{it})
}
\prod_{j=0, j\neq t}^{T_{i}}
\frac{1}{1 + \exp(\alpha_{i} + \eta_{it})} \right)
} \\
&= \cdots \\
&= \frac{
\exp(\eta_{iT_{i}})
}{
\sum_{t\in S_{i}} \exp(\eta_{it})
}
\end{aligned}
\end{equation}
Note the similarity in form to the likelihood for a Cox proportional hazards regression. The interpretation is similar too: the probability of dying on the day that you did, conditional on \textit{not} dying on the other days that you could have. The full log-likelihood for the case crossover model is then, assuming independence between subjects:
\begin{equation}\begin{aligned}\label{eqn:ccfulllik}
\ell(\theta) &= \sum_{i=1}^{n} \left( \eta_{iT_{i}} - \log\left( \sum_{t\in S_{i}} \exp(\eta_{it})\right) \right) \\
&= -\sum_{i=1}^{n}\log\left( \exp(-(\eta_{iT_{i}} - \eta_{i0}) + \cdots + \exp(-(\eta_{iT_{i}} - \eta_{iT_{i-1}})\right)
\end{aligned}\end{equation}
The second statement, involving the differences, will be useful in dealing with the constraint that $\sum_{t=1}^{T_{i}}p_{it} = 1$ for each $i$.


## Approximation methodology

### INLA

Integrated Nested Laplace Approximations (INLA; @inla) is a deterministic method for approximating the marginal posterior distribution of a large number of latent effects in a Bayesian heirarchical model. INLA considers heirarchical Bayesian models of the following form:

\begin{equation}\label{eqn:inla-spec}
\begin{aligned}
Y_{i} | W_{i},\theta_{2} &\sim \pi(y_{i}|w_{i},\theta_{2}), i = 1\ldots n \\
W | \theta_{1} &\sim \text{Normal}\left( 0,Q^{-1}(\theta_{1}) \right) \\
(\theta_{1},\theta_{2}) \equiv \theta &\sim \pi(\theta)
\end{aligned}
\end{equation}

$Y_{i}$ is the observed response, $W_{i}$ is an element of the (unobserved) \textit{latent field}, which could represent each subject's \textit{risk} towards whatever event $Y_{i}$ represents, and $W = (W_{1},\ldots,W_{n})$. $\theta$ collects all hyperparameters, which include those upon which the precision matrix $Q$ depends, and those upon which the likelihood depends. In the implementation, little distinction is made between the two sources of hyperparameters, but a major distinction is drawn between $\theta$ and $W$. 

The objects of primary interest here are the marginal posteriors of the elements of the latent field given the data, $\pi(w_{i}|y_{i})$, for which INLA provides fast, scalable approximations. In a structured additive model, $W$ is the collection of all the terms in the linear predictor, plus the linear predictors themselves. Specifically, INLA adds a noise variable onto the linear predictor itself,
\begin{equation}
\eta_{it} = \sum_{q=1}^{M_{1}}\beta_{q}z_{qi}(t) + \sum_{q=1}^{M_{2}}f_{q}(u_{qi}(t)) + Z_{it}
\end{equation}
with $Z_{it}\overset{i.i.d.}{\sim}\text{Normal}(0,\tau^{-1})$, with $\tau$ set to some large constant (the default in INLA is $e^{15}$). The latent field is the ordered collection
\begin{equation}
W = (\eta_{11},\ldots,\eta_{nT_{n}},f_{1}(u_{11}(1)),\ldots,f_{M_{2}}(u_{M_{2}n}(T_{n})),\beta_{1},\ldots,\beta_{M_{1}})
\end{equation}
of all linear terms and evaluations of smooth terms. This is given a joint Gaussian prior, with precision matrix $Q$. The key factor that makes the computations feasible is that $Q$ is a very sparse matrix. The "full" INLA proceeds by finding a set of integration points $\theta_{1},\ldots,\theta_{K}$ that interpolate the posterior $\pi(\theta|y)$, computing an efficient approximation to $\pi(W_{i}|\theta_{k},y)$, and then computing
\begin{equation}
\pi(W_{i}|y) = \int\pi(W_{i},\theta|y)d\theta \approx \sum_{k=1}^{n}\pi(W_{i}|\theta_{k},y)\pi(\theta_{k}|y)\Delta_{k}
\end{equation}
for each $i$, where $\Delta_{k}$ are weights chosen so that the resulting approximation integrates (sums) to $1$.

The full INLA is quite complex. Much of this complexity is added in order to make the resulting approximations more accurate in smaller samples. Since the primary motivation of the case-crossover model is to analyze large administrative databases, "higher-order" accuracy isn't a main concern in this work. In the next section, we develop an INLA-inspired approximation that retains the computational efficiency and generality of the INLA approach to approximating the posterior of interest in (\ref{eqn:inla-spec}), but is simple enough for the end-user to understand.

# Proposed Approach

## Overview

We first note that the case-crossover model with a structured additive predictor can be written in a heirarchical form \textit{nearly} compatible with (\ref{eqn:inla-spec}):
\begin{equation}\label{eqn:ccinlaspec}
\begin{aligned}
\pi(y_{i}|W,S_{i}) &= \frac{\exp(\eta_{iy_{i}})}{\sum_{t\in S_{i}} \exp(\eta_{it})} \\
W|\theta &\sim N(0,Q^{-1}(\theta)) \\
\theta &\sim \pi(\theta)
\end{aligned}
\end{equation}
Comparing with (\ref{eqn:inla-spec}), there are two differences, the first trivial and the second not:
\begin{enumerate}
\item The likelihood doesn't have any hyperparameters $\theta$, which simplifies the notation,
\item Importantly: $\pi(y_{i}|W,S_{i},z)$ depends on \textit{more than one} element of $W$.
\end{enumerate}
Notwithstanding the drawbacks discussed in section 1.3.3, this latter difference explains why we can't just fit the model with the existing INLA software; it's not directly compatible. The key quantities of interest for inference are
\begin{enumerate}
\item $\pi(W|y)$, the \textit{posterior distribution of the latent field}, and
\item $\pi(\theta|y)$, the \textit{posterior distribution of the hyperparameters}.
\end{enumerate}
Our approach for obtaining these quantities is to obtain a Gaussian approximation for $\pi(W|y)$, centred at the posterior mode $\hat{W} = \text{argmax}_{W}\pi(W,\theta|y) = \text{argmax}_{W}\pi(y|W,\theta)\pi(W|\theta)$, which is itself a function of $\theta$. We use a plug-in estimate of $\theta$ for this purpose, obtained as $\hat{\theta} = \text{argmax}_{\theta}\pi(\theta|y)$. This itself requires an approximation for $\pi(\theta|y)$, which we then also use for inference. To readers familiar with the R-INLA software, our approach is analagous to `strategy='gaussian'` and `int.strategy='eb'` in `INLA::inla`. We now elaborate on this approach.

## Explicit expressions for the precision matrix

So far in the description of the method, the precision matrix $Q = \text{var}(W|\theta)^{-1}$ of the Gaussian prior on the latent field is a mysterious object. Here we construct an explicit expression for it. Begin by writing the linear predictor $\eta$ in $m$-dimensional vector form, $\eta = (\eta_{11},\ldots,\eta_{n,T_{n}})$, giving
\begin{equation}\label{eqn:etarelation}
\eta = X\beta + AU + Z
\end{equation}
Here, 
\begin{enumerate}
\item $X$ is a (dense) design matrix as in a usual linear regression,
\item $\beta$ is a vector of regression coefficients, given a $\beta\sim\text{Normal}(0,\Sigma_{\beta})$ prior,
\item $A$ is a \textit{sparse} random effect design matrix, assigning random effects $U_{k}$ to observations, as in a usual mixed model,
\item $U\sim\text{Normal}(0,\Sigma_{U})$ is the vector of random effects, and
\item $Z\sim\text{Normal}(0,\tau^{-1}I)$ is this aformentioned extra noise term.
\end{enumerate}
In the notation of section 2, $U$ is the vector of $f_{q}(u_{qi}(t))$ "smooth" terms. This is an artifact of the Bayesian structured additive regression paradigm; no mathematical distrinction is made between classical "random effects" and smooth terms. Note the flexibility of this construction: $\Sigma_{U}$ can be specified to include random-walk smoothing, time series structures, correlated random effects, and so on. 

To actually get $Q$, now, directly use properties of jointly normal random variables. We want to characterize
\begin{equation}
W = \begin{pmatrix}\eta \\ U \\ \beta \end{pmatrix}\sim \text{Normal}\left(0,Q^{-1}\right) \\
\end{equation}
The relation (\ref{eqn:etarelation}) specifies the joint (unconditional) distribution of $(U,\beta)$, and the conditional distribution of $\eta | U,\beta$, as follows:
\begin{equation}\begin{aligned}
\eta | U,\beta &\sim \text{Normal}\left( AU + X\beta,\tau^{-1}I\right) \\
\begin{pmatrix} U \\ \beta \end{pmatrix} &\sim \text{Normal}\left( 0, \begin{pmatrix} \Sigma_{U} & 0 \\ 0 & \Sigma_{\beta}\end{pmatrix}\right)
\end{aligned}\end{equation}
from which we obtain:
\begin{equation}\begin{aligned}\label{eqn:varmat-1}
\begin{pmatrix} U \\ \beta \\ Z \end{pmatrix} \sim \text{Normal}\left( 0, \begin{pmatrix} \Sigma_{U} & 0 & 0 \\ 0 & \Sigma_{\beta} & 0 \\ 0 & 0 & \tau^{-1}I\end{pmatrix}\right)& \\
\begin{pmatrix} \eta \\ U \\ \beta \end{pmatrix} \overset{d}{=} \begin{pmatrix} A & X & I \\ I & 0 & 0 \\ 0 & I & 0 \\ \end{pmatrix}\begin{pmatrix} U \\ \beta \\ Z \end{pmatrix}
\equiv V\begin{pmatrix} U \\ \beta \\ Z \end{pmatrix}& \\
\sim \text{Normal}\left(0,V \begin{pmatrix} \Sigma_{U} & 0 & 0 \\ 0 & \Sigma_{\beta} & 0 \\ 0 & 0 & \tau^{-1}I\end{pmatrix}V^{T} \right) \\
\sim \text{Normal}\left(0,\begin{pmatrix} A\Sigma_{U}A^{T} + X\Sigma_{\beta}X^{T} + \tau^{-1}I & A\Sigma_{U} & X\Sigma_{\beta} \\ \Sigma_{U}A^{T} & \Sigma_{U} & 0 \\ \Sigma_{\beta}X^{T} & 0 & \Sigma_{\beta} \end{pmatrix} \right)&
\equiv \text{Normal}\left(0,\Sigma\right)&
\end{aligned}\end{equation}
The precision matrix is then obtained as
\begin{equation}\label{eqn:precmat-1}
Q = \Sigma^{-1} = \begin{pmatrix}
\tau I & -\tau A & -\tau X \\
-\tau A^{T} & \Sigma_{U}^{-1} + \tau A^{T}A & \tau A^{T}X \\
-\tau X^{T} & \tau X^{T}A & \Sigma_{\beta}^{-1} + \tau X^{T}X \\
\end{pmatrix}
\end{equation}
Though this expression looks messy, we note a few things:
\begin{enumerate}
\item $A$ is sparse and $X$ is dense; $X$ has a small number of columns, one for each linear term in the model. Combining these observations, we note that the dense matrices $X,X^{T}$, $A^{T}X$, $X^{T}A$, and $X^{T}X$ are of very small dimension, and the \textit{sparse} matrices $A,A^{T}$, $A^{T}A$ and $I$ are all of large dimension. The result is that $Q$ is very large and very sparse. Of course if the linear terms are mostly categorical variables then $X$ may also be sparse.
\item This is the whole story. Models with different linear and random effects/smooth term structures may have different $X, A, \Sigma_{\beta}$ and $\Sigma_{U}$, but the form for $Q$ remains the same across models in this framework. The exception is when there are no linear, or no smooth terms in the linear predictor. In this case, the form of $Q$ is the same, with the appropriate rows/columns removed.
\end{enumerate}
We will use this $Q$ in the approximations to the posteriors of the latent field and the hyperparameters.

## Gaussian approximation to the posterior of the latent field

To begin, we seek a Gaussian approximation to $\pi(W|y,\theta) = \pi(y|W,\theta)\pi(W|\theta)$, $\tilde{\pi}_{G}(W|y,\theta)$. Since $\pi(W|\theta)$ is already Gaussian, this will amount to a quadratic approximation to the log-likelihood. We have
\begin{equation}\begin{aligned}\label{eqn:wobjective}
\pi(W|y,\theta) &\propto \exp\left( -\frac{1}{2}W^{T}QW + \log\pi(y|W,\theta) \right) \\
&\approx \exp\left( -\frac{1}{2}W^{T}QW + -\frac{1}{2}(W-\hat{W})^{T}C(W-\hat{W}) \right) \\
&\propto \exp\left( -\frac{1}{2}(W-\hat{W})^{T}(Q+C)(W-\hat{W}) \right) \\
\end{aligned}\end{equation}
where $\log\pi(y|W,\theta) \approx \exp(-\frac{1}{2}(W-\hat{W})^{T}C(W-\hat{W}) )$ is a quadratic approximation to the log-likelihood centred at an appropriate point $\hat{W}$; we may add a $\hat{W}Q\hat{W}$ term into the exponent, as this is constant with respect to $W$ and so gets absorbed into the integrating constant. $C = -\frac{\partial^{2}}{\partial W\partial W^{T}}\log\pi(y|W,\theta)\vert_{W=\hat{W}}$ is minus the Hessian of the log-likelihood evaluated at $\hat{W}$. The mode of $\pi(W|y,\theta)$ is this point $\hat{W}$, which we have to find. Note that $\hat{W} \equiv \hat{W}(\theta)$ is a function of $\theta$, as it is the mode of $\pi(W|y,\theta)$ for a particular $\theta$.

As the latent field $W$ is usually very large-dimensional, optimizing (\ref{eqn:wobjective}) is potentially computationally prohibitive. The key is the sparsity of $Q$, which ensures a single Newton step requires only solving one sparse system. To see this, take a quadratic approximation to $\log\pi(y|W,\theta) = \sum_{i=1}^{n}\log\pi(y_{i}|W,\theta)$ at some initial value $W_{0}$,
\begin{equation}
\log\pi(y|W,\theta) \approx \log\pi(y|W_{0},\theta) + (W - W_{0})^{T}\nabla\log\pi(y|W_{0},\theta) - \frac{1}{2}(W - W_{0})^{T}C_{0}(W - W_{0}),
\end{equation}
where $\nabla\log\pi(y|W_{0},\theta)$ is the gradient evaluated at $W = W_{0}$ and $C_{0} = -\frac{\partial^{2}}{\partial W\partial W^{T}}\vert_{W=W_{0}}$ is the negative Hessian evaluated at the same point. Now, note that the quantity $-W_{0}^{T}QW_{0}$ doesn't depend on $W$, so we may add/subtract it in the constant, rearranging to obtain:
\begin{equation}
\log\pi(W|y,\theta) \approx -\frac{1}{2}(W-W_{0})^{T}\left(Q + C_{0}\right)(W - W_{0}) + (W - W_{0})^{T}\nabla\log\pi(y|W_{0},\theta)
\end{equation}
The gradient of this equation:
\begin{equation}
\nabla_{W}\log\pi(W|y,\theta) = -\left(Q + C_{0}\right)(W - W_{0}) + \nabla\log\pi(y|W_{0},\theta)
\end{equation}
is set to $0$ to obtain the update equation:
\begin{equation}\label{eqn:updatenewton}
\left(Q + C_{0}\right)(W_{1} - W_{0}) = \nabla\log\pi(y|W_{0},\theta)
\end{equation}
The matrix on the LHS is sparse and analytic expressions for $Q$, $C$, and $\nabla\log\pi(y|W_{0},\theta)$ are available. Hence, the only computational cost of an iteration is solving a sparse linear system. The ability to do this quickly is precisely why INLA works.

An important point here is that while this algorithm is \textit{efficient} by construction, there is absolutely nothing that guarantees that it is \textit{stable}. Instability is a well-known reason not to naively apply Newton's method in practice, especially in high-dimensions. In the implementation, we make use of the highly efficient and robust `IPOPT` software of @ipopt, which utilizes sparse matrix algorithms internally, retaining the problem-specific efficiency present here. However, the construction (\ref{eqn:updatenewton}) will be useful during the \textit{hyperparameter} optimization, to be discussed next.

The final Gaussian approximation to $\pi(W|y,\theta)$ is
\begin{equation}\label{eqn:finalgaussapprox}
\pi_{G}(W|y,\hat{\theta}) = \text{Normal}\left( \hat{W}(\hat{\theta}),Q(\hat{\theta}) + \hat{C}\right)
\end{equation}
where $\hat{\theta} = \text{argmax}_{\theta}\tilde{\pi}(\theta|y)$ and $\hat{W}(\hat{\theta}) = \text{argmax}_{W}\tilde{\pi}_{G}(W|y,\hat{\theta})$. We now turn to the task of finding $\hat{\theta}$.

## Approximation to the posterior of the hyperparameters

To begin, we describe the approximation to $\pi(\theta|y)$, $\tilde{\pi}(\theta|y)$. Taking the entire vector of latent variables $(\theta,W)$, the desired quantity can be viewed as the \textit{marginal} posterior of $\theta$. We use the method of @tierney. This is based off of Laplace approximations; see Appendix A for a brief review.

### Marginal posteriors: the method of Tierney and Kadane

In our present situation, we have data $y$, a large latent field $W$, and a small number of additional latent parameters $\theta$, and our object of interest is $\pi(\theta|y) = \int\pi(\theta,W|y)dW$. @tierney describe a method of obtaining highly accurate approximations to this density using Laplace approximations. We have
\begin{equation}
\pi(\theta|y) = \int_{\mathbb{R}^{m}}\pi(\theta,W|y)dW = \frac{\int_{\mathbb{R}^{m}}\pi(\theta,W,y)dW}{\int_{\mathbb{R}^{k}}\int_{\mathbb{R}^{m}}\pi(\theta,W,y)dWd\theta}
\end{equation}
where $k$ is the (small) dimension of $\theta$ and $m$ is the (large) dimension of $W$. @tierney apply a Laplace approximation to both the numerator and denominator separately. Their motivation in doing this is accuracy; they argue that while the error in naively applying this method is still relative $O(n^{-1})$, broken down it is actually the product of two terms, one for the integration constant and one for the "functional form". When renormalized via numerical integration, the relative error is $O(n^{-3/2})$, which is quite accurate. It is also imporant for the reader who is not an expert on Laplace approximations to observe that because the errors are \textit{relative}, not \textit{absolute}, the approximation here does not depend on the size of the quantity being approximated. When estimating tail probabilities and other small quantities this presents a substantial advantage over sampling-based methods, which typically have \textit{additive} error rates like $O(n^{-1/2})$.

To actually get this approximation, do Laplace twice. For the numerator, we have
\begin{equation}\begin{aligned}
\int_{\mathbb{R}^{m}}\pi(\theta,W,y)dW &= \pi(\theta)\int_{\mathbb{R}^{m}}\pi(W,y|\theta)dW \\
&\approx \pi(\theta) (2\pi)^{m/2}|Q(\theta) + \hat{C}(\theta)|^{-1/2}\pi(\hat{W}(\theta),y|\theta)
\end{aligned}\end{equation}
where $Q$ and $C$ are the usual precision and hessian matrices, the latter being evaluated at the conditional mode $\hat{W}(\theta) = \text{argmax}\pi(W,y|\theta)$ already found. It bears repeating that while this expression is complicated, every quantity needed here has already been computed in finding the Gaussian approximation to $\pi(W,y|\theta)$. 

For the denominator, it's a bit more complicated. We're now interested in $(\hat{W},\hat{\theta})$, the conditional mode of $\pi(\theta,W,y)$ with respect to both $W$ and $\theta$ jointly. Similarily, we need the $m+k$-dimensional Hessian matrix $D$ of (the log of) this function, evaluated at this mode. The approximation is then
\begin{equation}
\int_{\mathbb{R}^{k}}\int_{\mathbb{R}^{m}}\pi(\theta,W,y)dWd\theta \approx (2\pi)^{(m+k)/2}|\hat{D}|^{-1/2}\pi(\hat{\theta},\hat{W},y)
\end{equation}
The full approximation of @tierney is then
\begin{equation}\label{eqn:tkapprox}
\pi(\theta|y) \approx \pi(\theta) (2\pi)^{m/2}|Q(\theta) + \hat{C}(\theta)|^{-1/2}\pi(\hat{W}(\theta),y|\theta) \times (2\pi)^{-(m+k)/2}|\hat{D}|^{1/2}\pi(\hat{\theta},\hat{W},y)^{-1}
\end{equation}

### The INLA approach

Equation (\ref{eqn:tkapprox}) looks very complicated, because it is very complicated. We immediately note two things:
\begin{enumerate}
\item Most of the terms in (\ref{eqn:tkapprox}) are constant with respect to $\theta$.
\item All of the terms which are \textit{not} constant with respect to $\theta$ are already available as a result of the Gaussian approximation.
\end{enumerate}
With these points in mind, INLA uses the approximation
\begin{equation}\label{eqn:thetaapprox2}
\pi(\theta|y) \approx \frac{\pi(y|\hat{W}(\theta),\theta)\pi(\hat{W}(\theta)|\theta)\pi(\theta)}{\pi_{G}(W|y,\theta)}\vert_{W = \hat{W}(\theta)}
\end{equation}
@inla claim that this is equal to the TK approximation, "after renormalization", which means "up to constants". For purposes of optimization, the constants don't actually matter, and for inference, the low dimension of $\theta$ allows renormalization via numerical integration. 

This claim of equivalence is true, but it's not obvious as stated. To see it, note the following:
\begin{enumerate}
\item The $\pi(\hat{W}(\theta),y|\theta)$ term is common to (\ref{eqn:thetaapprox2}) and (\ref{eqn:tkapprox}), because $\pi(\hat{W}(\theta),y|\theta) = \pi(y|\hat{W}(\theta),\theta)\pi(\hat{W}(\theta)|\theta)$.
\item The Gaussian approximation in the denominator of (\ref{eqn:thetaapprox2}) is a Gaussian approximation \textit{with mean} $\hat{W}(\theta)$ \textit{evaluated at} this same point, $\hat{W}(\theta)$. \textit{So the exponential term cancels}---and the result is just $\pi_{G}(W|y,\theta)\vert_{W = \hat{W}(\theta)} = (2\pi)^{-m/2}|Q(\theta) + \hat{C}(\theta)|^{1/2}$.
\item Everything else in both expressions is constant.
\end{enumerate}
With these observations, (\ref{eqn:thetaapprox2}) evaluates to
\begin{equation}\begin{aligned}\label{eqn:thetaapprox}
\log\tilde{\pi}(\theta|y) &\approx c + \log\pi(y|W,\theta) + \log\pi(\hat{W}|\theta) + \log\pi(\theta) - \frac{1}{2}\log|Q(\theta) + \hat{C}(\theta)| \\
&= c + \log\pi(y|W,\theta) + \log\pi(\theta) + \frac{1}{2}\log|Q(\theta)| - \frac{1}{2}\hat{W}(\theta)^{T}Q(\theta)\hat{W}(\theta) - \frac{1}{2}\log|Q(\theta) + \hat{C}(\theta)|
\end{aligned}\end{equation}
We use the approximation (\ref{eqn:thetaapprox}) for $\tilde{\pi}(\theta|y)$ in our approach.

There is a subtle detail here that equation (\ref{eqn:thetaapprox}) glosses over: the conditional mode $\hat{W}(\theta)$ depends on $\theta$. This is extremely inconvenient; the objective $\pi(\theta|y)$ will be evaluated many times during the optimization, so having each evaluation itself depend on a very high-dimensional optimization (directly and through $\hat{C}$) sounds like a computational deal-breaker. This is where (\ref{eqn:updatenewton}) comes in: for a \textit{small change} in $\theta$, $\hat{W}(\theta)$ is not expected to change too much. Hence in moving from point $\theta_{1}$ to point $\theta_{2}$, performing a single or small number of Newton steps to find $\hat{W}(\theta_{2})$ \textit{starting from} $\hat{W}(\theta_{1})$ is efficient, and likely to be stable. Hence in the implementation, each evaluation of $\pi(\theta|y)$ involves a Newton-based optimization, but by making use of a lookup table of pre-computed $\theta\mapsto\hat{W}(\theta)$ mappings, stability and efficiency can be ensured.

## Marginal variances

With the approximation (\ref{eqn:finalgaussapprox}) in hand, posterior marginal means, standard deviations, and quantiles can be obtained from $\hat{W}$ and $Q(\hat{\theta}) + \hat{C}$. Specifically,
\begin{equation}\label{eqn:finalmargapprox}
W_{i}|y \overset{\cdot}{\sim} \text{Normal}\left( \hat{W},\left(Q(\hat{\theta}) + \hat{C}\right)^{-1}_{ii}\right)
\end{equation}
gives the user everything they need. However, we don't want to compute the entire inverse of $Q(\hat{\theta}) + \hat{C}$, as this matrix is quite large in general. All we need is the diagonals of its inverse. To compute these efficiently, we again use sparsity. Compute the cholesky decomposition of $Q(\hat{\theta}) + \hat{C}$:
\begin{equation}
Q(\hat{\theta}) + \hat{C} = \hat{L}\hat{L}^{T}
\end{equation}
where $\hat{L}$ is sparse and upper-triangular. Sparsity of the LHS means this operation can be performed in $O(n)$ time. It's much easier to invert $\hat{L}$ than $Q(\hat{\theta}) + \hat{C}$, because its upper triangular structure allows solving by back-substitution. Doing this yields the diagonal elements of the inverse of the precision matrix as
\begin{equation}\label{eqn:margvar}
...
\end{equation}
@inla specify a recursive equation for computing the desired diagonal elements from $\hat{L}$, which bypasses the need to invert $\hat{L}$ explicitly. Since computing marginal variances only needs to be done once, (\ref{eqn:margvar}) is sufficient for our purposes.

With (\ref{eqn:finalmargapprox}) in hand, we have our full approximation to the marginal posteriors of $W_{i}|y$, and hence for all regression coefficients, random effects, smooth terms, and of course the linear predictor itself. We now turn back to the case-crossover model.

## Calculations for the case-crossover model

### The likelihood

With the approximation methodology worked out in general for models of the form (\ref{eqn:inla-spec}), all that we would need to specify for a given implementation is the likelihood and its Hessian, and a prior for the hyperparameters. The latter is application-specific, since what $\theta$ contains depends on the structure of $\eta$. The likelihood, on the other hand, is model-specific. For the case-crossover, however, we have the added complication of having the non-linear constraint that $\sum_{t=1}^{T_{i}} p_{it} = \sum_{t=1}^{T_{i}}\frac{e^{\eta_{it}}}{1 + e^{\eta_{it}}} = 1$ for each $i$. We call this "non-linear" in reference to it being non-linear in $\eta_{it}$, not $p_{it}$. From (\ref{eqn:ccinlaspec}), we have
\begin{equation}\begin{aligned}
\log\pi(y|W) &= -\sum_{i=1}^{n}\log\left( \exp(-(\eta_{iT_{i}} - \eta_{i0}) + \cdots + \exp(-(\eta_{iT_{i}} - \eta_{iT_{i-1}})\right) \\
&= -\sum_{i=1}^{n}\log\left( \exp(-(\Delta_{i0}) + \cdots + \exp(-(\Delta_{i(T_{i}-1)})\right)
\end{aligned}\end{equation}
where we have defined $\Delta_{it} = \eta_{iT_{i}} - \eta_{it}$ for $i = 0,\ldots,T_{i}-1$ as the \textit{difference} in log-odds of exposure for control day $t$ compared to the case day $T_{i}$ for subject $i$. Inference is done on the $\Delta_{it}$ as a manner for dealing with the non-linear constraint. We may compute $\eta_{it}$ from $\Delta_{i} = (\Delta_{i1},\ldots,\Delta_{iT_{i}})$ by solving the non-linear equation
\begin{equation}
\frac{e^{\eta_{iT_{i}}}}{1 + e^{\eta_{iT_{i}}}} = 1 - \sum_{t=0}^{T-1}\frac{e^{\eta_{iT_{i}}}}{e^{\Delta_{it}} + e^{\eta_{iT_{i}}}}
\end{equation}
numerically for $\eta_{iT_{i}}$, and then subbing in $\eta_{it} = \eta_{iT_{i}} - \Delta_{it}$. Note though that in given applications, the $\Delta_{it}$ or a simple function of them are often the objects of primary inferential interest anyways, used to make statements such as "covariate xyz is associated with an increase of odds of mortality by a factor of $e^{\Delta_{it}}$", and so on.

### The precision matrix under a non-linear constraint

This does, however, change the precision matrix and Hessian. Convenient to our re-parameterization is that, while our \textit{constraint} is non-linear, $\Delta_{i}$ is a linear transformation of $\eta_{i}$,
\begin{equation}\begin{aligned}\label{eqn:Di}
\Delta_{i} &= \begin{pmatrix}\eta_{iT_{i}} - \eta_{i0} \\ \vdots \\ \eta_{iT_{i}} - \eta_{iT_{i-1}} \end{pmatrix} = D_{i}\eta_{i} \\
D_{i} &= \begin{pmatrix}
-1 & 0 & \cdots & 0 & 1 \\
0 & -1 & \cdots & 0 & 1 \\
  &    & \ddots &   &   \\
  &    &       & -1 & 1 \\
\end{pmatrix}
\end{aligned}\end{equation}
where the differencing matrix $D_{i}\in\mathbb{R}^{(T_{i}-1)\times T_{i}}$ has rank $T_{i}-1$. The functional form of each $D_{i}$ is exactly the same; all that changes is the dimension. This is applied to the whole vector $\eta$, which gives
\begin{equation}
\Delta = \begin{pmatrix} \Delta_{1} \\ \vdots \\ \Delta_{n}\end{pmatrix} = \begin{pmatrix} D_{1}\eta_{1} \\ \vdots \\ D_{n}\eta_{n} \end{pmatrix} = \begin{pmatrix}
D_{1} & &  \\
    & \ddots & \\
    &     & D_{n} \\
\end{pmatrix}
\begin{pmatrix} \eta_{1} \\ \vdots \\ \eta_{n} \end{pmatrix}
\equiv D\eta
\end{equation}
$D$ is the block-diagonal matrix of differencing matrices $D_{i}$, with dimension $\left(\sum_{i=1}^{n}T_{i} - n\right) \times \sum_{i=1}^{n}T_{i}$ and full row rank. It remains to reconstruct the latent field, from the joint distribution of $(\Delta,U,\beta)$. Let 
\begin{equation}\label{eqn:wcdef}
W_{c} = \begin{pmatrix} \Delta \\ U \\ \beta \end{pmatrix}
\end{equation}
be the latent field under our non-linear constraint on $\eta$. Let $X_{c} = DX$ and $A_{c} = DA$. Following the construction of the original $W$, we have
\begin{equation}\begin{aligned}
\Delta | U,\beta = D\eta | U, \beta &\sim \text{Normal}\left( A_{c}U + X_{c}\beta,\tau^{-1}DD^{T}\right) \\
\begin{pmatrix} U \\ \beta \end{pmatrix} &\sim \text{Normal}\left( 0, \begin{pmatrix} \Sigma_{U} & 0 \\ 0 & \Sigma_{\beta}\end{pmatrix}\right)
\end{aligned}\end{equation}
from which we obtain
\begin{equation}\begin{aligned}\label{eqn:varmat-1}
\begin{pmatrix} U \\ \beta \\ Z \end{pmatrix} \sim \text{Normal}\left( 0, \begin{pmatrix} \Sigma_{U} & 0 & 0 \\ 0 & \Sigma_{\beta} & 0 \\ 0 & 0 & \tau^{-1}I\end{pmatrix}\right)& \\
\begin{pmatrix} \Delta \\ U \\ \beta \end{pmatrix} \overset{d}{=} \begin{pmatrix} A_{c} & X_{c} & D \\ I & 0 & 0 \\ 0 & I & 0 \\ \end{pmatrix}\begin{pmatrix} U \\ \beta \\ Z \end{pmatrix}
\equiv V\begin{pmatrix} U \\ \beta \\ Z \end{pmatrix}& \\
\sim \text{Normal}\left(0,V \begin{pmatrix} \Sigma_{U} & 0 & 0 \\ 0 & \Sigma_{\beta} & 0 \\ 0 & 0 & \tau^{-1}I\end{pmatrix}V^{T} \right) \\
\sim \text{Normal}\left(0,\begin{pmatrix} A_{c}\Sigma_{U}A_{c}^{T} + X_{c}\Sigma_{\beta}X_{c}^{T} + \tau^{-1}DD^{T} & A_{c}\Sigma_{U} & X_{c}\Sigma_{\beta} \\ \Sigma_{U}A_{c}^{T} & \Sigma_{U} & 0 \\ \Sigma_{\beta}X_{c}^{T} & 0 & \Sigma_{\beta} \end{pmatrix} \right)&
\equiv \text{Normal}\left(0,\Sigma\right)&
\end{aligned}\end{equation}
The precision matrix is then obtained as
\begin{equation}\label{eqn:precmat-3}
Q = \Sigma^{-1} = \begin{pmatrix}
\tau \left( DD^{T}\right)^{-1} & -\tau \left( DD^{T}\right)^{-1}A_{c} & -\tau \left( DD^{T}\right)^{-1}X_{c} \\
-\tau A_{c}^{T}\left( DD^{T}\right)^{-1} & \Sigma_{U}^{-1} + \tau A_{c}^{T}\left( DD^{T}\right)^{-1}A_{c} & \tau A_{c}^{T}\left( DD^{T}\right)^{-1}X_{c} \\
-\tau X_{c}^{T}\left( DD^{T}\right)^{-1} & \tau X_{c}^{T}\left( DD^{T}\right)^{-1}A_{c} & \Sigma_{\beta}^{-1} + \tau X_{c}^{T}\left( DD^{T}\right)^{-1}X_{c} \\
\end{pmatrix}
\end{equation}
This has a very similar form to (\ref{eqn:precmat-1}), with the replacement of $A$ and $X$ with their differences versions $A_{c} = DA$, $X_{c} =DX$, and the addition of the $\left( DD^{T}\right)^{-1}$ terms. In order to understand if and how this affects the sparsity of $Q$, we need to examine the structure of $D$, $DD^{T}$, and $(DD^{T})^{-1}$. $D$ is a large block-diagonal matrices with $D_{i}$ on the blocks. This is quite sparse, as each $D_{i}$ has the form (\ref{eqn:Di}). The "transpose-crossproduct" $DD^{T}$ of $D$ is the block-diagonal matrix of transpose-crossproducts of the $D_{i}$:
\begin{equation}\label{eqn:thedifference}
DD^{T} = \begin{pmatrix}
D_{1}D_{1}^{T} & & \\
& \ddots & \\
& & D_{n}D_{n}^{T}
\end{pmatrix} ; 
(DD^{T})^{-1} = \begin{pmatrix}
(D_{1}D_{1}^{T})^{-1} & & \\
& \ddots & \\
& & (D_{n}D_{n}^{T})^{-1}
\end{pmatrix}
\end{equation}
Each $D_{i}D_{i}^{T}$ is dense with dimension $T_{i}-1$, equal to the number of control days. However, simple computation shows that its inverse has a particular form,
\begin{equation}
D_{i}D_{i}^{T} = J + I
\end{equation}
where $I$ is the identity and $J = 11^{T}$ is a matrix of ones of the appropriate dimension. Since $\text{rank}(J) = 1$ regardless of its dimension, $DD^{T}$ is a low-rank update of the identity and its inverse can be computed via a rank-one update to the identity. Specifically, using the Woodbury (or Sherman-Morrison) formula,
\begin{equation}\label{eqn:woodbury1}
(D_{i}D_{i}^{T})^{-1} = \left( I_{T_{i}-1} + 11^{T}\right)^{-1} = I - 1(1 + 1^{T}1)^{-1}1^{T} = I - \frac{1}{T_{i}}J
\end{equation}
This matrix is cheap to compute. The resulting $(DD^{T})^{-1}$ is still quite sparse, as each dense block is of dimension equal to the number of control days (i.e. $T_{i}-1$). In the uncommon event that the number of control days is so large as to render computation of $(DD^{T})^{-1}$ impractical or unfeasible, note that the factors $\frac{1}{T_{i}}J$ will then be very small, and a sparse approximation can be obtained simply by removing this term, approximating $(DD^{T})^{-1} \approx I$. In common applications, it is not expected that the number of control days is so large as for this to be necessary.

### The Hessian

The last "custom" ingredient for this model is (minus) the Hessian of the log-likelihood. With the difference-based construction (\ref{eqn:wcdef}), we compute the Hessian with respect to $\Delta_{i}$, not $\eta_{i}$. Noting the form of the log-likelihood as a sum-of-sums,
\begin{equation}
\log\pi(y|W) = -\sum_{i=1}^{n}\log\left( \exp(-(\Delta_{i0}) + \cdots + \exp(-(\Delta_{i(T_{i}-1)})\right)
\end{equation}
we see that $C$ will be a block-diagonal matrix with each block depending on $\Delta_{i0},\ldots,\Delta_{i(T_{i}-1)}$. We have
\begin{equation}\begin{aligned}
\frac{\partial}{\partial \Delta_{it}} \log\pi(y_{i}|W) &= \frac{e^{-\Delta_{it}}}{e^{-\Delta_{i0}} + \cdots + e^{-\Delta_{i(T_{i}-1)}}}\\\
\frac{\partial^{2}}{\partial \Delta_{it}^{2}} \log\pi(y_{i}|W) &= -\frac{e^{-\Delta_{it}}}{e^{-\Delta_{i0}} + \cdots + e^{-\Delta_{i(T_{i}-1)}}}\left(1 - \frac{e^{-\Delta_{it}}}{e^{-\Delta_{i0}} + \cdots + e^{-\Delta_{i(T_{i}-1)}}} \right) \\
\frac{\partial^{2}}{\partial \Delta_{it}\Delta_{is}} \log\pi(y_{i}|W) &= -\frac{e^{-(\Delta_{it} + \Delta_{is})}}{e^{-\Delta_{i0}} + \cdots + e^{-\Delta_{i(T_{i}-1)}}}\\
\end{aligned}\end{equation}
With
\begin{equation}
C_{i} = \begin{pmatrix}
\frac{\partial^{2}}{\partial \Delta_{i0}^{2}}\log\pi(y_{i}|W) & \frac{\partial^{2}}{\partial \Delta_{i0}\Delta_{i1}}\log\pi(y_{i}|W) & \cdots & \frac{\partial^{2}}{\partial \Delta_{i0}\Delta_{i(T_{i}-1)}}\log\pi(y_{i}|W) \\
\frac{\partial^{2}}{\partial \Delta_{i0}\Delta_{i1}}\log\pi(y_{i}|W) & \frac{\partial^{2}}{\partial \Delta_{i1}^{2}}\log\pi(y_{i}|W) & \cdots & \vdots \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^{2}}{\partial \Delta_{i0}\Delta_{i(T_{i}-1)}}\log\pi(y_{i}|W) & \cdots & \cdots & \frac{\partial^{2}}{\partial \Delta_{i(T_{i}-1)}^{2}}\log\pi(y_{i}|W)
\end{pmatrix}
\end{equation}
as the negative Hessian of the $i^{th}$ subject's contribution to the log-likelihood with respect to $\Delta_{i}$, the full $C$-matrix is
\begin{equation}\label{eqn:thehessian}
C = \begin{pmatrix}
C_{1} & & & & & \\
& \ddots & & & & \\
& & C_{n} & & & \\
& & & 0 & & \\
& & & & \ddots & & \\
& & & & & 0 \\
\end{pmatrix}
\end{equation}
The zeroes at the end are the derivatives with respect to the $U_{i}$ and $\beta$. This may seem peculiar, but it is an artifact of the structure of $\eta_{i}$ as a linear predictor "plus noise". The latent field is a function of \textit{both} $\eta_{it}$ and $U_{i}$, $\beta$; changing these latter two terms while holding $\eta_{it}$ fixed then does not change the likelihood, resulting in zero derivatives. This is consistent with the construction of the equivalent matrix in INLA. Each $C_{i}$ is dense, but of small dimension (again, equal to the number of control days). Further, comparing (\ref{eqn:thehessian}) and (\ref{eqn:thedifference}), we see that the additional denseness of $Q$ and $C$ occur \textit{in the same pattern}, so the added inefficiency does not compound when adding these two matrices.

# Appendix A: review of Laplace approximations {-}

Common in statistics are real-value functions $f_{n}:\mathbb{R}^{k}\to\mathbb{R}$ depending on some parameter $n$ (usually a sample size), with the following properties:
\begin{enumerate}
\item $f_{n}$ has a single global maximum, or if many local extrema, one local max which "dominates" (is bigger than the rest).
\item As $n\to\infty$, $f_{n}$ becomes more and more peaked around this dominating mode, and hence becomes better and better approximated by a quadratic centred at this point.
\end{enumerate}
The best example is the log-likelihood function $f_{n}(x) \equiv \sum_{i=1}^{n}\ell(x_{i})$; as $n$ gets larger, the CLT ensures that this function becomes more and more peaked about its mean. Approximating a log-likelihood using a quadratic is equivalent to approximating a density using a Gaussian, which partly explains why this latter approach is so popular.

Laplace approximation is closely related to this idea, in the context of approximating \textit{integrals} of such functions. Suppose for such a function we want to estimate
\begin{equation}
I = \int_{\mathbb{R}^{k}} e^{f_{n}(x)}dx
\end{equation}
\textit{Laplace's method} works as follows:
\begin{enumerate}
\item Find the mode of $f_{n}$, call it $\hat{x}$.
\item Taylor-expand $f_{n}$ about $\hat{x}$, obtaining $f_{n}(x) \approx f_{n}(\hat{x}) - \frac{1}{2}(x - \hat{x})^{T}H(\hat{x})(x - \hat{x})$. $H$ is minus the Hessian, $H = -\partial^{2}/\partial x \partial x^{T} f_{n}(x)$, evaluated at $\hat{x}$.
\item The exponential of this is a Gaussian density, which we know how to integrate.
\end{enumerate}
The result is the \textit{Laplace approximation to the integral}, given by
\begin{equation}
\hat{I} \approx (2\pi)^{k/2}|H(\hat{x})|^{1/2}e^{f_{n}(\hat{x})}
\end{equation}
A good question to ask is: why? Why Taylor expand inside the expoential, why not the whole thing? An obvious reason is computational: typically the kinds of functions that concentrate around their modes are made up of sums of things, and these are much easier to work with than their exponentials, which are products. But a better, and subtler, reason is how the approximation errors work. When taking a raw Taylor expansion of a function, the error is \textit{additive}, of the form $|f(x) - \hat{f}(x)| = O((x-\hat{x})^{3})$, for example. But when taking the \textit{exponential} of a Taylor approximation, the error becomes \textit{relative}, which can offer huge advantages when e.g. using an approximate density to approximate a tail probability, or some other really small number. It can be shown through tedious calculation that the error in the Laplace approximation is $\hat{I} = I(1 + O(n^{-1}))$. Compared to the \textit{additive} $O(n^{-1/2})$ error rate attained by most sampling procedures, Laplace approximations are much better suited to approximating quantities which are likely to be small, like tail probabilities.


# References {-}

