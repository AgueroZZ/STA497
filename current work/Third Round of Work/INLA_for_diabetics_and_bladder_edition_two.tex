\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Approximate Bayesian Inference for Semi-parametric Proportional Hazard Models},
            pdfauthor={Ziang Zhang},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Approximate Bayesian Inference for Semi-parametric Proportional Hazard
Models}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Ziang Zhang}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{11/09/2019}


\begin{document}
\maketitle

\hypertarget{survival-analysis-model}{%
\section{Survival Analysis Model:}\label{survival-analysis-model}}

\hypertarget{introduction-to-survival-analysis}{%
\subsection{Introduction to Survival
Analysis:}\label{introduction-to-survival-analysis}}

In survival analysis, we are analyzing data-sets in which the response
variable of interest is the time until the occurrence of a particular
event, such as the lifetimes of patients with a specific kind of
disease, the durability of a bunch of light-bulbs etc. More
specifically, we may want to study the relationship between the
lifetimes of patients with the types of medicine they are using, to
conclude whether a certain type of medicine does improve the overall
survival times of patients. To put it in a more general setting, the
response variable T should be a non-negative random variable, and in
most cases, it should be a continuous random variable, defined over the
interval \([0,\infty)\). Let the probability density function of T be
denoted as \(f(t)\) and its cumulative distribution function be
\(F(t)\), then the survivor function \(S(t)\) of T can be defined as:
\begin{equation}\begin{aligned}\label{eqn:survivor}
S(t) = P(T > t) = \int_{t}^{\infty} f(x) dx
\end{aligned}\end{equation} Notice that \(S(t)\) is the probability of
an observation to survive to time t, and therefore it should be a
monotone decreasing function with \(S(0) = 1\) and
\(S(\infty) = \lim_{t\to\infty} S(x) = 0\). In survival analysis study,
we are mostly interested in the instantaneous rate of occurrence at a
specific time \(t\) given that the event does not occur before \(t\).
That instantaneous rate will be measured using a hazard function
\(h(t)\) which is defined as:
\begin{equation}\begin{aligned}\label{eqn:hazard}
h(t) = \lim_{s\to 0} \frac{P(t\le T \le t+s |T\ge t)}{s} = \frac{f(t)}{S(t)} = -\frac{\partial}{\partial t}\text{log}[S(t)]
\end{aligned}\end{equation} And the related cumulative hazard function
\(H(t)\) will be defined by:
\begin{equation}\begin{aligned}\label{eqn:chazard}
H(t) = \int_{0}^{t} h(u) du = -\text{log}[S(t)]
\end{aligned}\end{equation}

Often time, there will be \(some\) survival times in the data-set that
we cannot observe their exact values due to censoring or truncation,
which causes a great difficulty for us to carry out our analysis. I will
present the details of these problems at the section below.

\hypertarget{types-of-censoring-and-truncation}{%
\subsection{Types of Censoring and
Truncation:}\label{types-of-censoring-and-truncation}}

In survival analysis, we are mainly dealing with the problem of
right-censoring, interval-censoring and left truncation. Right-censoring
is when an individual's lifetime \(T_i\) is not exactly known because
the individual is still alive when the study terminates at \(C_i\), so
we are only sure about that \(T_i > C_i\) but not sure what exactly
\(T_i\) is. Interval-censoring on the other hand, rises when the
survival time is only known to be in an interval \((L_i,U_i)\), and the
left truncation problem happens when some survival times are not
recorded unless they are bigger than a specified start time \(t^{tr}\),
so all the data with survival times less than \(t^{tr}\) are missed.

In general, we use the term ``censoring'' to refer to the scenarios
where some lifetimes are only known to exceed their cutting times
\(C_i\), but we do not know how long do they last exactly. On the other
hand, the term ``truncation'' mostly refer to a data collected problem
where only lifetimes greater than the start time \(t^{tr}\) are
collected and observed. So these terms should be used in different
situations depending on what kind of survival data are we dealing with.

There are two types of right-censoring that appear most frequently in
the context of survival analysis, which are Type-I and Type-II
right-censoring.

Type-I right-censoring occurs when each individual's censoring time
\(C_i\) is fixed and known beforehand. That means when we collect a
bunch of survival times, we know whether each survival time is
right-censored and when is it censored exactly. In this case, we will be
able to write our original data-set \(\{ T_i,C_i:i=1,...,n \}\) as
\(\{ t_i,\delta_i:i=1,...,n \}\) where:

\begin{equation}\begin{aligned}\label{eqn:transformed data}
t_i = \text{min}\{T_i,C_i \},  \qquad  \delta_i = I(T_i \leq C_i)
\end{aligned}\end{equation}

This is the most common type of right-censoring, and we will focus on
this type of censoring for the rest of the passage.

Type-II right-censoring occurs when we only observed the r smallest
survival times in our sample. So the survival times that we can observed
will be like \(t_{(1)}<t_{(2)}<...<t_{(r)}\), and the other survival
times will be censored so we don't know the exact numbers. In this
scenario, we have a censoring time \(t_{(r)}\) that is itself random.

Lastly, independent random censoring happens when both the ith survival
time \(T_i\) and the ith censoring time \(C_i\) are random variable that
are independent.

\hypertarget{cox-proportional-hazard-model}{%
\subsection{Cox Proportional Hazard
Model:}\label{cox-proportional-hazard-model}}

In most survival analysis study, we are interested in incorporating some
covariates \(\tilde{X} =\{X_1,X_2,...,X_p\}\) into the distribution of
survival time \(T\), and studying their effects on the survival time
\(T\). Therefore, we often need to use different kinds of models to
specify the dependence of T on \(\tilde{X}\), and among those models,
the proportional hazard model introduced by Cox(1972) is the most
popular choice.

Let \(h(t|\tilde{x})\) denote the hazard function of \(T\) at time t for
a subject with covariates \(\tilde{x} = (x_1,x_2,...,x_p)\). The Cox
Proportional Hazard Model can be specified as follows:
\begin{equation}\begin{aligned}\label{eqn:CoxHazardModel}
h(t|\tilde{x}) = h_0(t)\text{exp}(\beta_1x_1+...+\beta_px_p)
\end{aligned}\end{equation} where \(h_0(t)\) is an arbitrary baseline
hazard function that does only depend on time, and \(\beta_i\)'s are the
unknown parameters that we are interested in estimating. The reason that
it is called a ``proportional'' hazard model is because for any two
subjects, the ratio of their hazard function will be constant over time.
This is a very strict assumption which should be checked before adopting
this model.

Notice that the baseline hazard function is left to be arbitrary, which
implies that the Cox Proportional Hazard Model will be a semi-parametric
model. There are different ways to define the baseline hazard functions,
and the piece-wise constant baseline hazard model will be the most
convenient and popular choice. We will focus on this kind of model in
the rest of this paper, and I will introduce it in details in the next
section.

\hypertarget{proportional-hazard-model-with-piece-wise-constant-baseline-hazard}{%
\subsection{Proportional Hazard Model with Piece-wise Constant Baseline
Hazard:}\label{proportional-hazard-model-with-piece-wise-constant-baseline-hazard}}

Firstly, we break the time axis into K intervals with endpoints
\(0=s_0<s_1<...<s_K < \text{max}\{t_i:i=1,...,n\}\), and assumes that
the baseline hazard function is constant in each interval. i.e:
\(h_0(t) = \lambda_k\) for \(t\in(s_{k-1},s_k), \ k=1,2, ...,K\) Let
\(\eta_{ik} = \text{log}(\lambda_k)+\beta_1x_{i1}+...\beta_px_{ip}\),
the model that we will be focusing on will be the semi-parametric
proportional hazard model, specified at below:
\begin{equation}\begin{aligned}\label{eqn:phmodel}
h(t_i) &= h_0(t_i)\text{exp}(\beta_1x_{i1}+...\beta_px_{ip})\\
       &= \text{exp}[\text{log}(\lambda_k)+\beta_1x_{i1}+...\beta_px_{ip}] \qquad t_i\in(s_{k-1},s_k] \\
       &= \text{exp}(\eta_{ik})
\end{aligned}\end{equation}

Using this information, we can easily derive the likelihood for that
single observation to be:
\begin{equation}\begin{aligned}\label{eqn:singlelike}
L &= f(t_i)^{\delta_i}S(t_i)^{(1- \delta_i)}\\
  &= h(t_i)^{\delta_i}S(t_i) \\
  &= \text{exp}(\delta_i \eta_{ik})\bigg\{ \text{exp} \big[ -\int_{0}^{t_i} h(u) du\big ] \bigg\} \\
  &= \text{exp}(\delta_i \eta_{ik})\bigg \{\text{exp}\big[-\sum_{j=1}^{k-1} (s_{j}-s_{j-1})\text {exp} (\eta_{ij}) - (t_i-s_{k-1})\text{exp}(\eta_{ik})\big]\bigg \}\\
\end{aligned}\end{equation}

Therefore, the full-likelihood of the data-set will be:
\begin{equation}\begin{aligned}\label{eqn:fulllike}
L &=  \prod_{i=1}^{n} \text{exp}(\delta_i \eta_{ik_{(i)}}) \text{exp} \bigg\{-\sum_{j=1}^{k_{(i)}-1} (s_{j}-s_{j-1})\text{exp}(\eta_{ij})-(t_i - s_{k_{(i)}-1})\text{exp}(\eta_{ik_{(i)}})\bigg \} \\
  &= \prod_{i=1}^{n} \text{exp} \big \{\delta_i \eta_{ik_{(i)}} -\sum_{j=1}^{k_{(i)}-1} (s_{j}-s_{j-1})\text{exp}(\eta_{ij})-(t_i - s_{k_{(i)}-1})\text{exp}(\eta_{ik_{(i)}}) \big \}
\end{aligned}\end{equation}

I emphasize the subscript for \(k_{(i)}\) because each survival time
will correspond to a different value of k, depending on which interval
the survival time lies in.

By taking the logarithm, the log-likelihood function for the \(i^{th}\)
observation \(t_i \in (s_{k-1},s_k]\) can be written as :
\begin{equation}\begin{aligned}\label{eqn:loglike}
l &= \text{log}[f(t_i)^{\delta_i}S(t_i)^{(1-\delta_i)}] \\
  &= \text{log}[h(t_i)^{\delta_i}S(t_i)]\\
  &= \delta_i \eta_{ik} - (t_i-s_{k-1})\text{exp}(\eta_{ik})-\sum_{j=1}^{k-1} [(s_{j}-s_{j-1})\text{exp}(\eta_{ij})]
\end{aligned}\end{equation}

Similarly, the full log-likelihood can be derived as:
\begin{equation}\begin{aligned}\label{eqn:fullloglike1}
l = \sum_{i=1}^{n} \bigg \{ \delta_i \eta_{ik_{(i)}} - (t_i-s_{k_{(i)}-1})\text{exp}(\eta_{ik_{(i)}})-\sum_{j=1}^{{k_{(i)}}-1} [(s_{j}-s_{j-1})\text{exp}(\eta_{ij})]\bigg \}
\end{aligned}\end{equation}

It can see from the above expression that by considering a piece-wise
constant baseline hazard, we make the corresponding log-likelihood much
easier to work with, since the integral \(\int_{0}^{t_i} h(u)du\) can be
replaced by a sum.

\newpage

\hypertarget{inlas-inference-methodology}{%
\section{INLA's Inference
Methodology:}\label{inlas-inference-methodology}}

\hypertarget{data-augmentation-using-poisson-likelihood}{%
\subsection{Data Augmentation Using Poisson
Likelihood:}\label{data-augmentation-using-poisson-likelihood}}

Here the INLA algorithm cannot directly be applied, because if we look
at the log-likelihood of a single survival time \(\{t_i,\delta_i\}\), we
can find that it depends on more than one \(\eta\). To use INLA, we
required a conditional independent latent field together with a sparse
Hessian matrix for the log-likelihood. That means we need to make sure
that for a single data point, the log-likelihood should be free of terms
from latent field once we condition on one of the term from the latent
field.

To solve this puzzle, we will utilize a data ``augmentation'' trick to
transform the log-likelihood of a single data point into the form that
INLA likes. Notice that if we are looking at a random variable \(X_i\)
that follows a Poisson distribution with mean
\((t_i-s_{k-1})\text{exp}(\eta_{ik})\), then the log-likelihood
corresponding to a single data point \(\{X_i = 0 \}\) will be:
\begin{equation}\begin{aligned}\label{eqn:loglike1}
l &= \text{log}\bigg \{P\big [X_i =0|\lambda = (t_i-s_{k-1})\text{exp}(\eta_{ik}) \big]\bigg \}\\
  &= 0\times \text{ln}[(t_i-s_{k-1})\text{exp}(\eta_{ik})] - (t_i-s_{k-1})\text{exp}(\eta_{ik}) - \text{ln}(0!)\\
  &= - (t_i-s_{k-1})\text{exp}(\eta_{ik})
\end{aligned}\end{equation} Similarly, when \(X_i = 1\), the
log-likelihood of this single data point is:
\begin{equation}\begin{aligned}\label{eqn:loglike2}
l &= \text{log}\bigg(P(X_i =1|\lambda = (t_i-s_{k-1})\text{exp}(\eta_{ik}))\bigg)\\
  &= 1\times \text{ln}((t_i-s_{k-1})\text{exp}(\eta_{ik})) - (t_i-s_{k-1})\text{exp}(\eta_{ik}) - \text{ln}(1!)\\
  &= \text{ln}(t_i-s_{k-1})+\eta_{ik}-(t_i-s_{k-1})\text{exp}(\eta_{ik})\\
  &\propto \eta_{ik}-(t_i-s_{k-1})\text{exp}(\eta_{ik})
\end{aligned}\end{equation}

Here we can basically ignore the term \(\text{ln}(t_i-s_{k-1})\) as it
does not depend on any term from the latent field. So when we later take
derivative, this term will just disappear which means it won't affect
our C matrix.

We showed that the first two terms of the log-likelihood of a single
data point \(\{t_i,\delta_i\}\) can be viewed as the log-likelihood of a
single data point
\(X_i\sim \text{Poisson}\big[\lambda =(t_i-s_{k-1})\text{exp}(\eta_{ik})\big]\)
being \(0\) when \(\delta_i = 0\) and being 1 when \(\delta_i = 1\).

Next step will be to figure out a similar way to deal with the last term
in equation (3). Notice that for a Poisson random variable \(Y_j\) with
mean \((s_{j}-s_{j-1})\text{exp}(\eta_{ij})\), the log-likelihood for
observing it being \(0\) will be:
\begin{equation}\begin{aligned}\label{eqn:loglike3}
l &= \text{log}\bigg \{P\big [Y_j =0|\lambda = (s_{j}-s_{j-1})\text{exp}(\eta_{ij})\big ]\bigg \}\\
  &= -(s_{j}-s_{j-1})\text{exp}(\eta_{ij})
\end{aligned}\end{equation}

Similarly, if we gather a sample of
\(\{Y_{i_{1}}=0,Y_{i_{2}} =0, ..., Y_{i_{k}}=0 \}\) where each
\(Y_{i_j} \sim \text{Poisson}\big[\lambda = (s_{j}-s_{j-1})\text{exp}(\eta_{ij})\big]\)
is independent of others, then the log-likelihood of this sample will
simply be the sum of log-likelihood of each term due to independence,
which sums to be
\(\sum_{j=1}^{k-1} (s_{j}-s_{j-1})\text{exp}(\eta_{ij})\),that is
exactly what we want.

Putting these two pieces information together, which means if we have a
sample being
\(\{X_i =\delta_i,Y_{i_{1}}=0,Y_{i_{2}} =0, ..., Y_{i_{k}}=0 \}\), and
all the terms in this sample being mutually independent, then the
log-likelihood of this sample will just be the log-likelihood of the
single data point \(\{t_i,\delta_i\}\). Doing this for all the data
points \(\{t_i,\delta_i|i=1,...,n\}\). We retrieve the original
log-likelihood from the log-likelihood of a sample of
\(\sum_{i=1}^{n}{k_{(i)}}\) number of independent, but non-identical
Poisson random variables. In other words, we augment our original
data-set \(\{t_i,\delta_i|i=1,...,n\}\) into a huge
data-set\(\{x_i ,y_{i_{1}},y_{i_{2}}, ..., y_{i_{k_{(i)}}}|i=1,2,...n\}\),
where all the terms in this new data-set are mutually independent. This
is the cure for our problem since the log-likelihood of each term from
this new ``augmented'' data-set, will only depend on the latent field
through one \(\eta\).

\hypertarget{derivation-of-the-negated-hessian-matrix}{%
\subsection{Derivation of the Negated Hessian
Matrix:}\label{derivation-of-the-negated-hessian-matrix}}

Here I will present how the Bayesian approximation can be carried out
using an INLA-type of algorithm. Firstly, to make the covariance matrix
of the joint Gaussian latent field non-singular, and to simplify the
Hessian matrix that we are going to derive later, we will assume that
for each \(\eta_{ij}\), a normal random noise \(\epsilon_{ij}\) is
added. We assume that \(\epsilon_{ij} \sim N(0,\sigma^2_{\epsilon})\)
being mutually independent across different i and j. In other words, we
will write
\(\eta_{ij} = \text{log}(\lambda_k)+\beta_1x_{i1}+...\beta_px_{ip} + \Gamma_i+\eta_{ij}\),
where \(\Gamma_i\) is any random effect that we believe exists in the
context of the study.

Then, the latent field can be denoted as:
\begin{equation}\begin{aligned}\label{eqn:field1}
\tilde W = \big[\eta_{11},\eta_{12},...,\eta_{1k},\eta_{2k},...,\eta_{nk},\Gamma_1,...,\Gamma_q,\beta_1,...,\beta_p,\text{log}(\lambda_1),...,\text{log}(\lambda_k)\big]^T\\
\end{aligned}\end{equation}

Besides assume that \(\tilde W\) is a GMRF, we also assume that
\(\text{log}(\lambda_{k+1})-\text{log}(\lambda_k)\) follows
\(N(0,\tau^{-1})\), a RW1 model. So we will just use \(\tilde \theta\)
to denote the hyper-parameter vector that determines the precision
matrix of our latent field.

Now, let's derive the negated Hessian matrix of the log-likelihood with
respect to the latent field. To do that, let's first consider the
log-likelihood consider only one survival time \(\{t_i,\delta_i\}\)
where \(t_i \in (s_{k_{(i)}-1},s_{k_{(i)}}]\). In this case, the
log-likelihood for this data point will be:
\begin{equation}\begin{aligned}\label{eqn:loglikeagain}
l = \delta_i \eta_{ik_{(i)}} - (t_i-s_{k_{(i)}-1})\text{exp}(\eta_{ik_{(i)}})-\sum_{j=1}^{k_{(i)}-1} [(s_{j}-s_{j-1})\text{exp}(\eta_{ij})]
\end{aligned}\end{equation} The derivative with respect to
\(\eta_{ik_{(i)}}\) will be\\
\begin{equation}\begin{aligned}\label{eqn:hessian}
\frac{\partial l}{\partial \eta_{ik_{(i)}}}= \delta_i -(t_i-s_{k_{(i)}-1})\text{exp}(\eta_{ik_{(i)}})
\end{aligned}\end{equation} That means the negated second derivative
will be: \begin{equation}\begin{aligned}\label{eqn:hessian1}
-\frac{\partial^2 l}{\partial {\eta_{ik_{(i)}}}^2} = (t_i-s_{k_{(i)}-1})\text{exp}(\eta_{ik_{(i)}})
\end{aligned}\end{equation}

For first and negated second derivatives with \(\eta_{ij}\) where
\(j<k_{(i)}\), we have:
\begin{equation}\begin{aligned}\label{eqn:hessian2}
\frac{\partial l}{\partial \eta_{ij}}= -(s_{j}-s_{j-1})\text{exp}(\eta_{ij})\\
-\frac{\partial^2 l}{\partial {\eta_{ij}}^2} = (s_{j}-s_{j-1})\text{exp}(\eta_{ij})
\end{aligned}\end{equation}

Apparently, for \(\eta_{ij}\) where \(j>k_{(i)}\), we have the second
derivatives of log-likelihood being 0's. Combine them together, we know
that the negated Hessian matrix for the log-likelihood of
\(\{t_i,\delta_i\}\), \(H_i\) will be: \begin{equation}
\begin{bmatrix}
(s_1-s_0)\text{exp}(\eta_{i1})  & 0  & 0 & \cdots & \cdots & \cdots & 0 \\
0  & (s_2-s_1)\text{exp}(\eta_{i2})  & 0  & \ddots & && &  \\
0 & 0  & \ddots &   & \ddots & &  &  \\
\vdots & \cdots & \cdots & (s_{k_{(i)}-1}-s_{k_{(i)}-2})\text{exp}(\eta_{i(k_{(i)}-1)}) & \ddots & \vdots &  &  \\
\vdots & & \ddots & 0 & (t_i-s_{k_{(i)}-1})\text{exp}(\eta_{ik_{(i)}}) & \cdots & \vdots& \\
\vdots  & & & \ddots &   & \ddots  &  \vdots\\
\vdots  & && & \cdots & \cdots & \vdots\\
0 & \cdots &  \cdots & \cdots & \cdots & \cdots & 0\\
\end{bmatrix}
\end{equation} This is a very sparse matrix with only diagonal terms.

Repeating this procedure for the rest data points, using the property of
independence, we can get the negated Hessian matrix H for the full
log-likelihood will be: \begin{equation}
H = \begin{pmatrix} 
H_1 & 0 & 0 & \cdots & & \\ 
0 & H_2 & 0 & \cdots & & \\
  & \cdots & \ddots &  & & \\
& & & & H_n & \cdots & \vdots \\ 
& & & \ddots & &&\vdots \\
& & & & & & 0
\end{pmatrix}
\end{equation}

Here we build a block diagonal matrix H using each block \(H_i\)
obtained from above procedures. The negated Hessian matrix is very
sparse, which is exactly what we want it to be. Then, we will try to
derive the precision matrix Q of the latent field (To be continued).

\hypertarget{proposed-methodology-for-approximation}{%
\section{Proposed Methodology for
Approximation:}\label{proposed-methodology-for-approximation}}

In the paper ``Approximate Bayesian Inference for Case-Crossover
Models'', the author suggested a new type of algorithm to do the
approximation while allowing the log-likelihood of each observation to
be dependent on more than one element from the latent field, which means
the ad-hoc method using ``data augmentation'' is no longer needed
(Stringer,2019). Here we will demonstrate how that algorithm can be used
to estimate the parameters in Cox Proportional Hazard Model, and when
this new algorithm will be preferred than INLA's algorithm.

\hypertarget{approximation-using-partial-likelihood-with-right-censoring-only}{%
\subsection{Approximation using Partial Likelihood with Right censoring
only:}\label{approximation-using-partial-likelihood-with-right-censoring-only}}

For simplicity, let's assume that our main interest is the \(\beta_i\)'s
in the model but not the baseline hazard \(h_0(t)\), and the only type
of censoring present is right-censoring. Assume that
\(\{t_i:i=1,...,k\}\) is a set of k distinct lifetimes that we actually
\(observed\), such that \(t_{(1)} < t_{(2)} < ... <t_{(k)}\), and the
result n-k lifetimes are the censored lifetimes that are not observed.
Let \(R_i = R(t_{(i)})\) be the set of individuals who are alive and
uncensored prior to time \(t_{(i)}\) (including the i-th individual who
dies at \(t_{(i)}\)).

Define the hazard function for the i-th individual to be
\(h_0(t)\text{exp}(\eta_i)\), and let \(\Delta_{i,j}\) =
\(\eta_i -\eta_j\), then the partial likelihood for Cox Proportional
Hazard Model can be written as:
\begin{equation}\begin{aligned}\label{eqn:partial}
L(\beta) &= \prod_{i=1}^{k} \bigg\{\frac{\text{exp}[\eta_{(i)}]}{{\sum_{l\in R_i}^{}\text{exp}[\eta_{(l)}]}}\bigg \} \\
         &= \prod_{i=1}^{k} \bigg\{\frac{1}{{\sum_{l\in R_i}^{}\text{exp}[\eta_{(l)}-\eta_{(i)}]}}\bigg \} \\
         &= \prod_{i=1}^{k} \bigg\{\frac{1}{{\sum_{l\in R_i}^{}\text{exp}[-\Delta_{il}]}}\bigg \} \\
         &= \prod_{i=1}^{k} \bigg\{\frac{1}{{1 + \sum_{l\in R_i , l \neq i}^{}\text{exp}[-\Delta_{i,l}]}}\bigg \} \\
\end{aligned}\end{equation}

Notice that this partial likelihood does not include any information on
the baseline hazard function \(h_0(t)\), meaning that all of the
information are used to estimate the regression parameters in the model,
which hopefully should result in a more precise estimation for them.
Here it is obvious that the partial likelihood only depend on those
``differenced linear predictors'' \(\Delta_{i,j}\), so our latent field
in this case will be \(\{ \Delta,\beta,\Gamma \}\). More importantly,
because we are not estimating those baseline hazards, the algorithm's
convergence rate will be much faster. INLA does not allow this type of
approximation because using partial likelihood to ignore the baseline
hazard invalidates the ``Poisson data-augmentation'' trick that INLA
does to make the C-matrix diagonal. While non-diagonal C matrix is not
feasible in INLA's algorithm, it will be feasible in the new proposed
algorithm.

\hypertarget{derivation-of-hessian-matrix-and-precision-matrix}{%
\subsubsection{Derivation of Hessian matrix and Precision
matrix:}\label{derivation-of-hessian-matrix-and-precision-matrix}}

For i-th observation, the partial log-likelihood will be:
\begin{equation}\begin{aligned}\label{eqn:ithPart}
l &= -\text{ln}(1+{\sum_{j\in R_i, j \ne i}^{}\text{exp}[-\Delta_{i,j}]}) \\
\end{aligned}\end{equation}

Therefore, taking derivative with respect to \(\Delta_{iw}\), we can
get: \begin{equation}\begin{aligned}\label{eqn:derivofPart}
\frac{\partial l}{\partial \Delta_{i,w}} &= -\frac{\text{exp}(-\Delta_{i,w})}{1+{\sum_{j\in R_i, j \ne i}^{}\text{exp}[-\Delta_{i,j}]}} \\
\end{aligned}\end{equation}

Similarly, we can see that:

\begin{equation}\begin{aligned}\label{eqn:secderivofPart}
\frac{\partial^2 l}{\partial \Delta_{i,w}^2} &= \frac{\text{exp}(-\Delta_{i,w})\big[1+ \sum_{j\in R_i,j\ne i}\text{exp}(-\Delta_{i,j})\big]-\text{exp}(-2\Delta_{i,w})}{\big[1+\sum_{j\in R_i,j \ne i}\exp(-\Delta_{i,j})\big]^2} \\
\\
                                            &= \frac{\text{exp}(-\Delta_{i,w})}{1+ \sum_{j\in R_i,j\ne i}\text{exp}(-\Delta_{i,j})} \bigg\{1-\frac{\text{exp}(-\Delta_{i,w})}{1+ \sum_{j\in R_i,j\ne i}\text{exp}(-\Delta_{i,j})}\bigg \} \\
\end{aligned}\end{equation}

Suppose that \(M\ne w \ , \ M\neq i\) and \(M\in R_i\), then we also
have:

\begin{equation}\begin{aligned}\label{eqn:secderivofPart3}
\frac{\partial^2 l}{\partial \Delta_{i,w} \Delta_{i,M}} &= \frac{-\text{exp}(-\Delta_{i,w})*\big[-\text{exp}(-\Delta_{i,M})\big]}{\big[1+\sum_{j\in R_i,j \ne i}\exp(-\Delta_{i,j})\big]^2}\\
   \\
                                                      &= \frac{\text{exp}\big[-(\Delta_{i,w}+\Delta_{i,M})\big]}{\big[1+\sum_{j\in R_i,j \ne i}\text{exp}(-\Delta_{i,j})\big]^2}
\end{aligned}\end{equation}

In this case, the latent field will be:
\begin{equation}\begin{aligned}\label{eqn:latenforpartial}
\tilde{W} &= (\Delta_{1,2},\Delta_{1,3},...,\Delta_{1,n},\Delta_{2,3},\Delta_{2,4},...,\Delta_{n-1,n},...)^T \\
          &= (\tilde{\Delta}_1,\tilde{\Delta}_2,\tilde{\Delta}_3,...\tilde{\Delta}_{n-1},...)^T
\end{aligned}\end{equation} where each \(\tilde{\Delta}_i\) is defined
as \((\Delta_{i,i+1},\Delta_{i,i+2},...,\Delta_{i,n})^T\).

Using this notation, we can write the the negated hessian matrix \(C_i\)
of the i-th observation's log-likelihood, with respect to
\(\tilde{\Delta}_i\) being: \begin{equation}
C_i = \begin{pmatrix} 
-\frac{\partial^2 l}{\partial \Delta_{i,i+1}^2} & -\frac{\partial^2 l}{\partial \Delta_{i,i+1} \Delta_{i,i+2}} & -\frac{\partial^2 l}{\partial \Delta_{i,i+1} \Delta_{i,i+3}} & \cdots & & & -\frac{\partial^2 l}{\partial \Delta_{i,i+1} \Delta_{i,n}} \\ 
 & -\frac{\partial^2 l}{\partial \Delta_{i,i+2}^2} & -\frac{\partial^2 l}{\partial \Delta_{i,i+2} \Delta_{i,i+3}} & \cdots & & & -\frac{\partial^2 l}{\partial \Delta_{i,i+2} \Delta_{i,n}} \\
  &  & \ddots &  & & \\
& & & &  & & \vdots \\ 
& & & &  & & \\
& & & & & & -\frac{\partial^2 l}{\partial \Delta_{i,n}^2}\\
\end{pmatrix}
\end{equation}

Using these \(C_i\)'s as blocks, we can construct the \(C\) matrix for
the full data-set being: \begin{equation}
C = \begin{pmatrix} 
C_1 & 0 & 0 & \cdots & & \\ 
0 & C_2 & 0 & \cdots & & \\
  & \cdots & \ddots &  & & \\
& & & & C_k & \cdots & \vdots \\ 
& & & \ddots & &&\vdots \\
& & & & & & 0
\end{pmatrix}
\end{equation}

The C matrix is block diagonal, but each block \(C_i\) is not diagonal.
The reason for the C matrix to be block diagonal is that the
log-partial-likelihood of i-th observation only depends on the latent
field through the vector \(\tilde{\Delta}_i\), and for those
\(\tilde{\Delta}_j\) where \(j>k\), they are not even included in the
full log-partial-likelihood, so their corresponding \(C_j\) matrixes
will be all zeroes. Though the C matrix is very sparse, but it is not a
diagonal matrix, so INLA cannot handle this type of problem. However,
our proposed algorithm could easily handle it, because diagonality of C
matrix is not required here.

\hypertarget{adjustments-of-likelihood-for-tied-observations}{%
\subsubsection{Adjustments of Likelihood for tied
observations}\label{adjustments-of-likelihood-for-tied-observations}}

Throughout our discussion on how to use partial likelihood for doing
approximate Bayesian Inference, we made an important assumption that the
uncensored lifetimes are all unique, in other words
\(t_{(i)} \ne t_{(j)}\) for all \(i\neq j\). However, in real life
applications, there will be some cases where this assumption is not met.
For example, we might use a ``discretized'' measure of lifetime when we
are recording both 12.11 hours and 12.12 hours as 12.1 hours in the
study of sustainability of a certain type of battery. That means, we
should be able to deal with ``ties'' in our observed lifetimes. Here we
will focus on Efron and Breslow's methods of approximation.

From now, let's assume that for an observed lifetime \(t_j\), there are
\(d_{(j)}\) number of other observed lifetimes are ``tied'' with it. Let
\(D_{(j)}\) be the set of all the individuals with lifetimes tied at
\(t_j\), that means there will be \(d_{(j)}\) number of elements in
\(D_{(j)}\).

Breslow's method of approximation will be the easiest and fastest method
to use, when accuracy of estimations is not extremely required. The main
idea of Breslow approximation is to include all the events occured at
\(t_j\) in the risk set \(R(t_j)\). Therefore, we can write the partial
likelihood at \(t_j\) as:

\begin{equation}\begin{aligned}\label{eqn:tie1forBres}
L_j^B = \frac{\prod_{i \in D_{(j)}} \text{exp}(\eta_i)}{\big[ \sum_{i\in R_{j}} \text{exp}(\eta_i) \big]^{d_{(j)}}}
\end{aligned}\end{equation}

The use of Breslow method is really convenient, because its partial
likelihood has the same form of our ordinary partial likelihood when all
the observations are distinct. When the number of ties in the data-set
is not a lot, Breslow's method does give a fairly accurate result, and
its computation time is exactly the same as no ties in the set. However,
despite its appealing efficiency, if too many ties occuring at the same
time, then its accuracy will be influenced comparatively heavily. In
that case, we would like to consider an alternative method, which is
Efron method of approximation.

Efron initially suggested this method as an approximation for the case
when these ties are ``exactly continuous''. However, Efron's method in
general gives very accurate estimates, and is computationally fast
enough, even when the number of ties is very large. The main idea of
this approach is to give a partial approximation to the contribution to
\(R(t_j)\) of each tied occurence, and it uses a partial likelihood in
the form below:

\begin{equation}\begin{aligned}\label{eqn:tie1forEf}
L_j^{Ef} = \frac{\prod_{i \in D_{(j)}} \text{exp}(\eta_i)}{\prod_{h = 1}^{d_{(j)}} \bigg \{\sum_{i\in R_{j}} \text{exp}(\eta_i) - \frac{h-1}{d_j}\sum_{k\in D_{j}} \text{exp}(\eta_k) \bigg \}}
\end{aligned}\end{equation}

Although the partial likelihood of Efron's approximation is not as easy
as the partial likelihood of Breslow's method, it is still not bad for
our computational efficiency. In the case of a lot of ties occuring at a
specific time, it gives a more accurate result than Breslow's method.

There are other exact methods to deal with ties such as D.Cox's ``exact
discrete'' method and Kalbfleisch and Prentice's ``exact continuous''
method, but they are computationally too heavy to be used for our
purpose. So we choose to not use them for now.

\hypertarget{approximation-using-full-likelihood-with-left-truncation}{%
\subsection{Approximation using full-likelihood with
left-truncation:}\label{approximation-using-full-likelihood-with-left-truncation}}

If there are both right-censoring and left-truncations in our data-set,
the ``data augmentation trick'' that INLA uses actually still works
theoretically, but the software does not actually allow the user to run
the approximation under this scenario. Fortunately, it can be solved
using the package of the new proposed algorithm.

Recall that left-truncation happens when we cannot observed the i-th
individual lifetime \(t_i\), unless it is greater than the entry time
\(u_i\). Under this setup, all the observed lifetimes \(t_i\)'s are
known to be greater than their corresponding entry times \(u_i\)'s. In
other words, we should use conditional probability given \(t_i > u_i\)
to form our likelihood. For simplicity, let's still consider the same
semi-parametric proportional hazard model with piece-wise constant
basline hazard.

Denote the i-th lifetime as \(t_i\), the i-th left truncation time is
\(u_i\), and assume that \(t_i \in (s_{k_{(i)}-1},s_{k_{(i)}}]\),
\(u_i \in (s_{m_{(i)}-1},s_{m_{(i)}}]\). Therefore, we have the
likelihood being:

\begin{equation}\begin{aligned}\label{eqn:lt}
L &= \prod_{i=1}^{n} {\bigg[\frac{f(t_i)}{S(u_i)}\bigg]^{\delta_i}\bigg[\frac{S(t_i)}{S(u_i)}\bigg]^{1- \delta_i}} \\
  &= \prod_{i=1}^{n} {f(t_i)^{\delta_i}}{\frac{S(t_i)^{1-\delta_i}}{S(u_i)}} \\
  &= \prod_{i=1}^{n} {h(t_i)^{\delta_i}}{\frac{S(t_i)}{S(u_i)}} \\
\end{aligned}\end{equation}

Using the likelihood above, we can easily derive the log-likelihood
being:

\begin{equation}\begin{aligned}\label{eqn:lt_log}
l &= \sum_{i=1}^{n} {{\delta_i}\text{log}[h(t_i)]} + \sum_{i=1}^{n} {\big[\text{log}S(t_i)-\text{log}S(u_i)\big]}  \\
\end{aligned}\end{equation}

Recall that if \(t_i \in (s_{k_{(i)}-1},s_{k_{(i)}}]\), and
\(u_i \in (s_{m_{(i)}-1},s_{m_{(i)}}]\), we have the followings:
\begin{equation}\begin{aligned}\label{eqn:ltcase1}
\text{log}S(t_i) &= -\int_{0}^{t_i} h(x) dx \\
                 &= -\sum_{j=1}^{k_{(i)}-1} (S_j-S_{j-1})\text{exp}(\eta_{ij})-(t_i-S_{k_{(i)}-1})\text{exp}(\eta_{ik_{(i)}}) \\
\end{aligned}\end{equation}

Similarly:

\begin{equation}\begin{aligned}\label{eqn:ltcase2}
\text{log}S(u_i) &= -\int_{0}^{u_i} h(x) dx \\
                 &= -\sum_{j=1}^{m_{(i)}-1} (S_j-S_{j-1})\text{exp}(\eta_{ij})-(u_i-S_{m_{(i)}-1})\text{exp}(\eta_{im_{(i)}}) \\
\end{aligned}\end{equation}

Therefore, the difference between this two terms can be written as:
\begin{equation}\begin{aligned}\label{eqn:diff}
\text{log}S(t_i)-\text{log}S(u_i) = -\int_{u_i}^{t_i} h(x) dx \\
\end{aligned}\end{equation} Whereas:
\begin{equation}\begin{aligned}\label{eqn:diff2}
-\int_{u_i}^{t_i} h(x) dx = -\sum_{j=m_{(i)}+1}^{k_{(i)}-1} (S_j-S_{j-1})\text{exp}(\eta_{ij}) - (S_{m_{(i)}}-u_i)\text{exp}(\eta_{im_{(i)}})-(t_i - S_{k_{(i)}-1})\text{exp}(\eta_{n_{ik_{(i)}}})
\end{aligned}\end{equation}

Then, combine all of the information above together, we can derive an
expression for the log-likelihood of the sample:
\begin{equation}\begin{aligned}\label{eqn:all_together_logoftr}
l = \sum_{i=1}^{n} {\delta_i}{\eta_{i{k_{(i)}}}}-\sum_{i=1}^{n}\sum_{j=m_{(i)}+1}^{k_{(i)}-1}(S_j-S_{j-1})\text{exp}(\eta_{ij}) - \sum_{i=1}^{n} (S_{m_{(i)}}-u_i)\text{exp}(\eta_{im_{(i)}}) - \sum_{i=1}^{n} (t_i-S_{k_{(i)}-1})(\eta_{ik_{(i)}})
\end{aligned}\end{equation}

If we have \(m_{(i)} \leq k_{(i)}-1\), then the above expression
simplify to:
\begin{equation}\begin{aligned}\label{eqn:all_together_logoftr2}
l = \sum_{i=1}^{n} {\delta_i}{\eta_{i{k_{(i)}}}}- \sum_{i=1}^{n} (S_{m_{(i)}}-u_i)\text{exp}(\eta_{im_{(i)}}) - \sum_{i=1}^{n} (t_i-S_{k_{(i)}-1})(\eta_{ik_{(i)}})
\end{aligned}\end{equation}

Next step, I will derive the corresponding C and Q matrix in this case.

\hypertarget{derivation-of-c-matrix-with-left-truncation}{%
\subsubsection{Derivation of C-matrix with
left-truncation}\label{derivation-of-c-matrix-with-left-truncation}}

To make the derivation most general, I will assume that
\(k_{(i)}-1 \geq m_{(i)}+1\), since otherwise the computation will be
simplified to trivial. For the i-th observation \(t_{(i)}\) with
left-truncation time \(u_{(i)}\), assume that:
\(t_i \in (s_{k_{(i)}-1},s_{k_{(i)}}]\), and
\(u_i \in (s_{m_{(i)}-1},s_{m_{(i)}}]\), then the likelihood of this
observation will be:
\begin{equation}\begin{aligned}\label{eqn:ind_i_log}
l = \delta_i \eta_{ik_{(i)}} - \sum_{m_{(i)}+1}^{k_{(i)}-1} (S_j - S_{j-1}) \text{exp}(\eta_{ij}) - (S_{m_{(i)}}-u_{(i)})\text{exp}(\eta_{im_{(i)}})-(t_{(i)}-S_{k_{(i)}-1})\text{exp}(\eta_{ik_{(i)}})
\end{aligned}\end{equation}

For \(j<m_{(i)}\) or \(j>k_{(i)}\), apparently we have
\(\frac{ \partial l}{\partial \eta_{ij}}\) =
\(\frac{ \partial^2 l}{\partial \eta_{ij}^2}\) = 0.

For \(j = m_{(i)}\), we can compute that
\(\frac{ \partial l}{\partial \eta_{ij}}\) =
\(\frac{ \partial^2 l}{\partial \eta_{ij}^2}\) =
\(- (S_{m_{(i)}}-u_{(i)})\text{exp}(\eta_{im_{(i)}})\).

For \(m_{(i)} < j < k_{(i)}\),
\(\frac{ \partial l}{\partial \eta_{ij}}\) =
\(\frac{ \partial^2 l}{\partial \eta_{ij}^2}\) =
\(- (S_j-S_{j-1})\text{exp}(\eta_{ij})\).

For \(j = k_{(i)}\), it can be shown that:

\begin{equation}\begin{aligned}\label{eqn:smalladd}
\frac{ \partial l}{\partial \eta_{ij}} = \delta_i - (t_{(i)}-S_{k_{(i)}-1})\text{exp}(\eta_{ik_{(i)}}) \\
\end{aligned}\end{equation} So,
\begin{equation}\begin{aligned}\label{eqn:smalladd2}
\frac{ \partial^2 l}{\partial \eta_{ij}^2} = - (t_{(i)}-S_{k_{(i)}-1})\text{exp}(\eta_{ik_{(i)}})
\end{aligned}\end{equation}

From now, let's denote \(\text{exp}(\eta_{ij})\) as \(b_{ij}\). Now, we
can use the above information, to obtain the C-matrix (negated Hessian)
of the i-th observation:

\begin{equation}
\begin{pmatrix} 
0 &   &  & \cdots & \cdots &  & \cdots \\
0 & \ddots  &   &  & && &  \\
\vdots & \cdots & (S_{m_{(i)}}-u_{(i)}) b_{im_{(i)}} &  & \cdots & \cdots & \cdots &  \\
\vdots & & &(S_{m_{(i)}+1}-S_{m_{(i)}})b_{i(m_{(i)}+1)} &  & \cdots & \cdots& \\
\vdots & & & \ddots  & \ddots & \ddots & \cdots& \\
\vdots  & & & \cdots & (S_{k_{(i)}-1}-S_{k_{(i)}-2})b_{i(k_{(i)}-1)}  & \cdots &  \\
\vdots  & && & \cdots & (t_i-S_{k_{(i)}-1})b_{ik_{(i)}} & \cdots\\
0 & \cdots &  \cdots & \cdots & \cdots & \cdots & \ddots \\
\end{pmatrix}
\end{equation}

Let's call the C-matrix of observation i as \(C_i\), then the C-matrix
of the whole sample will be: \begin{equation}
C = \begin{pmatrix} 
C_1 & 0 & 0 & \cdots & & \\ 
0 & C_2 & 0 & \cdots & & \\
  & \cdots & \ddots &  & & \\
& & & & C_n & \cdots & \vdots \\ 
& & & \ddots & &&\vdots \\
& & & & & & 0
\end{pmatrix}
\end{equation}

We can see that the present of left-truncation does not change the
overall shape of the C-matrix. The only effect of it is to change the
diagonal terms of each individual observation's \(C_i\) matrix depending
on the i-th left-truncation time. Therefore, the computation efficiency
and precision will not be affected too much.

\newpage

\hypertarget{approximation-using-full-likelihood-with-interval-censoring}{%
\subsection{Approximation using full-likelihood with
interval-censoring:}\label{approximation-using-full-likelihood-with-interval-censoring}}

Suppose that we are not observing the exact lifetimes of individiuals,
but only the set of intervals that contain each lifetime. In other
words, our data-set is \(\{L_i, R_i ; i=1,...n\}\) , where
\(L_i \leq t_i \leq R_i\). Using the same way to define the piece-wise
constant hazard functions as before, we can assume that for the i-th
observation, we have \(R_i\in(S_{k_{(i)}-1},S_{k_{(i)}}]\), and
\(L_i\in(S_{m_{(i)}-1},S_{m_{(i)}}]\).

Now, we can write down the log-likelihood of the i-th individual using
the above information:

\begin{equation}\begin{aligned}\label{eqn:icl1}
l_i &= \delta_i \text{log}\big[h_i(R_i)\big] - \int_{0}^{R_i} h_i(u) du + \text{log}\bigg\{ 1-\text{exp}\big[ -\int_{L_i}^{R_i} h_i(u)du \big]  \bigg\} \\
    &= \delta_i \eta_{ik_{i}} - \sum_{j=1}^{k_{(i)}-1} \text{exp}(\eta_{ij}) - (R_i-S_{k_{(i)}-1}) \text{exp}(\eta_{ik_{(i)}}) + \text{log}\big[1-\text{exp}(\vartheta_i)\big] \\
\end{aligned}\end{equation}

Where \(\vartheta_i\) is defined as:
\begin{equation}\begin{aligned}\label{eqn:vartheta}
\vartheta_i = -\int_{L_i}^{R_i} h_i(u)du = -\sum_{j=m_{(i)}+1}^{k_{(i)}-1} (S_j - S_{j-1})\text{exp} (\eta_{ij}) -(S_{m_{(i)}} - L_i)\text{exp}(\eta_{im_{(i)}})-(R_i - S_{k_{(i)}-1})\text{exp}(\eta_{ik_{(i)}})
\end{aligned}\end{equation}

Now, we can take derivative of this log-likelihood with respect to the
ij-th linear predictor (assume that \(m_{(i)}+1\leq j \leq k_{(i)}-1\)),
and get the following result:

\begin{equation}\begin{aligned}\label{eqn:ijderivofsingle}
\frac{\partial l_i}{\partial \eta_{ij}} = -(S_j-S_{j-1})\text{exp}(\eta_{ij}) -  \frac{\text{exp}(\vartheta_i) \frac{\partial \vartheta_i}{\partial \eta_{ij}}}{1-\text{exp}(\vartheta_i)}
\end{aligned}\end{equation}

Where \(\frac{\partial \vartheta_i}{\partial\eta_{ij}}\) is
\(-(S_j-S_{j-1})\text{exp}(\eta_{ij})\) in this case.

Since \(\vartheta_i\) depends on more than one linear predictors, so it
follows naturally that \(\frac{\partial l_i}{\partial \eta_{ij}}\) will
be a function of several different linear predictors, which means the
C-matrix of each observation's log-likelihood will be non-diagonal. That
will be a serious problem for INLA, as its package relies on the
diagonality of C-matrix, but it will not cause any probelm for our new
proposed algorithm, as the diagonality of C-matrix is no longer required
here.

\hypertarget{example-from-diabetics-data-set}{%
\section{Example from Diabetics
Data-set}\label{example-from-diabetics-data-set}}

Firstly, I will use the data-set ``diabetics'' to demonstrate the
equivalence between ``coxph'' approach in survival package, INLA's
approach and our proposed approach. This data-set contains the results
from a trail of laser coagulation for the treatment of diabetic
retinopathy from 197 patients. Each patient had one eye randomized to
laser treatment and the other eye received no treatment. (To be
continued on ``proposed approach'')

The variable ``id'' specifies the subject's ID.

The variable ``laser'' is a categorical variable with levels xenon or
argon.

The variable ``age'' is the age of the subject at diagnosis.

The variable ``eye'' is a categorical variable with levels left or
right.

The variable ``trt'' is a categorical variable with levels 0 for no
treatment and 1 for treatment using laser.

The variable ``risk'' classifies the risk levels of the patients.

The response variable in this data-set will be ``time'', which are the
actual time to blindness in months, minus the minimum possible time to
event (6.5 months), and ``status'' indicates whether the time is
censored with 1 for visual loss and 0 for censored. The censoring can be
due to death, dropout, or end of the study.

Let use briefly view the structure of ``diabetics'':

\hypertarget{data-set}{%
\subsection{Data-set:}\label{data-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(}\KeywordTok{as_tibble}\NormalTok{(diabetic))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 8
##      id laser   age eye     trt  risk  time status
##   <int> <fct> <int> <fct> <int> <int> <dbl>  <int>
## 1     5 argon    28 left      0     9  46.2      0
## 2     5 argon    28 right     1     9  46.2      0
## 3    14 xenon    12 left      1     8  42.5      0
## 4    14 xenon    12 right     0     6  31.3      1
## 5    16 xenon     9 left      1    11  42.3      0
## 6    16 xenon     9 right     0    11  42.3      0
\end{verbatim}

We can see that those survival times are right-censored. We will fit a
cox proportional hazard model with piece-wise constant baseline hazard,
assuming that each individual will have the same baseline hazard
function. The variable ID will be treated as a random effect(which can
be added using code ``frailty.gaussian(id)''). The variables ``age'',
``eye'', ``trt'' and ``laser'' will be included as fixed effects.

\hypertarget{survival-coxph}{%
\subsection{Survival: coxph}\label{survival-coxph}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diabetic.CoxPh <-}\StringTok{ }\KeywordTok{coxph}\NormalTok{(}\KeywordTok{Surv}\NormalTok{(time, status)}\OperatorTok{~}\NormalTok{age }\OperatorTok{+}\StringTok{ }\NormalTok{eye }\OperatorTok{+}\StringTok{ }\NormalTok{trt }\OperatorTok{+}\StringTok{ }\NormalTok{laser }\OperatorTok{+}\StringTok{ }\KeywordTok{frailty.gaussian}\NormalTok{(id), }\DataTypeTok{data =}\NormalTok{ diabetic)}
\KeywordTok{summary}\NormalTok{(diabetic.CoxPh)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## coxph(formula = Surv(time, status) ~ age + eye + trt + laser + 
##     frailty.gaussian(id), data = diabetic)
## 
##   n= 394, number of events= 155 
## 
##                      coef      se(coef) se2      Chisq  DF    p      
## age                   0.009548 0.01323  0.009879   0.52  1.00 4.7e-01
## eyeright              0.483005 0.17501  0.168693   7.62  1.00 5.8e-03
## trt                  -1.007507 0.17930  0.174315  31.57  1.00 1.9e-08
## laserargon           -0.182388 0.39471  0.293566   0.21  1.00 6.4e-01
## frailty.gaussian(id)                             131.35 79.63 2.4e-04
## 
##            exp(coef) exp(-coef) lower .95 upper .95
## age           1.0096     0.9905    0.9837    1.0361
## eyeright      1.6209     0.6169    1.1503    2.2842
## trt           0.3651     2.7388    0.2569    0.5189
## laserargon    0.8333     1.2001    0.3844    1.8062
## 
## Iterations: 6 outer, 24 Newton-Raphson
##      Variance of random effect= 0.9447455 
## Degrees of freedom for terms=  0.6  0.9  0.9  0.6 79.6 
## Concordance= 0.867  (se = 0.867 )
## Likelihood ratio test= 228  on 82.62 df,   p=2e-15
\end{verbatim}

From the output above, it can be seen that variables ``age'' and
``eyeright'' have positive association with the rate of occurrence of
visual loss, and variables ``trt'' and whether using ``argon'' type of
laser are negatively associated with the rate. All the fixed effects
that we included in this study have significant effects for the risk of
visual loss.

\hypertarget{bayesian-inla}{%
\subsection{Bayesian: INLA}\label{bayesian-inla}}

Now we fit the same model using INLA:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula =}\StringTok{ }\KeywordTok{inla.surv}\NormalTok{(time, status) }\OperatorTok{~}\StringTok{ }\NormalTok{age }\OperatorTok{+}\StringTok{ }\NormalTok{eye }\OperatorTok{+}\StringTok{ }\NormalTok{trt }\OperatorTok{+}\StringTok{ }\NormalTok{laser }\OperatorTok{+}\StringTok{ }\KeywordTok{f}\NormalTok{(id, }\DataTypeTok{model =} \StringTok{"iid"}\NormalTok{)}
\NormalTok{diabetic.INLA <-}\StringTok{ }\KeywordTok{inla}\NormalTok{(formula, }\DataTypeTok{control.compute =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{dic =} \OtherTok{TRUE}\NormalTok{), }\DataTypeTok{family =} \StringTok{"coxph"}\NormalTok{, }
                      \DataTypeTok{data =}\NormalTok{ diabetic, }\DataTypeTok{control.hazard=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{model=}\StringTok{"rw2"}\NormalTok{, }\DataTypeTok{n.intervals=}\DecValTok{20}\NormalTok{))}
\NormalTok{diabetic.INLA}\OperatorTok{$}\NormalTok{summary.fixed}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                     mean         sd  0.025quant     0.5quant  0.975quant
## (Intercept) -4.763714766 0.24994735 -5.27248742 -4.757676379 -4.28888200
## age          0.007155338 0.01084963 -0.01384758  0.007020824  0.02905144
## eyeright     0.385529001 0.17630568  0.04925320  0.381993816  0.74245455
## trt         -0.866913332 0.19229114 -1.26377664 -0.860048265 -0.50832653
## laserargon  -0.118719347 0.32276583 -0.77598169 -0.111745831  0.49837939
##                     mode          kld
## (Intercept) -4.746027234 3.989541e-06
## age          0.006833499 7.795531e-06
## eyeright     0.375067540 5.514554e-06
## trt         -0.845836419 1.338137e-05
## laserargon  -0.099011609 3.561565e-06
\end{verbatim}

It seems like these two approaches are similar enough. Though in the
classic ``coxph'' approach, the effects of age and using argon-type
laser are significant, but INLA gives insignificant results(the 95\%
credible interval contains zero). There is an estimate for intercept in
the INLA's method because we used a random walk prior in that.

\hypertarget{example-from-bladder-data-set}{%
\section{Example from Bladder
Data-set:}\label{example-from-bladder-data-set}}

Next, we will study the two approaches on the data-set ``bladder1''.
This is the full data-set that contains the result from a study on
recurrences of bladder cancer from 118 subjects. In this data-set, the
variables that we are interested in are ``id'', ``number'', ``size'',
``recur'', ``times'' and ``censored''.

The variable ``id'' is just the patient ID.

The variable ``number'' specifies initial number of tumors of each
subject.

The variable ``size'' is the size of largest initial tumor.

The variable ``recur'' is the number of recurrence of bladder cancer for
that subject.

The response variable will be ``time'' which is computed to be the
duration of times until recurrence or death, censored by the variable
``censored'' with 0 means being censored.

\hypertarget{data-set-1}{%
\subsection{Data-set:}\label{data-set-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{(bladder1)}
\NormalTok{data <-}\StringTok{ }\KeywordTok{select}\NormalTok{(data,}\OperatorTok{-}\KeywordTok{c}\NormalTok{(rsize,rtumor,enum))}
\NormalTok{data <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{censored =}\NormalTok{ status}\OperatorTok{==}\DecValTok{0}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(data}\OperatorTok{$}\NormalTok{censored)) \{}
  \ControlFlowTok{if}\NormalTok{(data}\OperatorTok{$}\NormalTok{censored[i]) data}\OperatorTok{$}\NormalTok{censored[i] <-}\StringTok{ }\DecValTok{0}
  \ControlFlowTok{else}\NormalTok{ data}\OperatorTok{$}\NormalTok{censored[i] <-}\StringTok{ }\DecValTok{1}
\NormalTok{\}}
\NormalTok{data <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{times =}\NormalTok{ stop}\OperatorTok{-}\NormalTok{start)}
\KeywordTok{head}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 10
##      id treatment number  size recur start  stop status censored times
##   <int> <fct>      <int> <int> <int> <int> <int>  <dbl>    <dbl> <int>
## 1     1 placebo        1     1     0     0     0      3        1     0
## 2     2 placebo        1     3     0     0     1      3        1     1
## 3     3 placebo        2     1     0     0     4      0        0     4
## 4     4 placebo        1     1     0     0     7      0        0     7
## 5     5 placebo        5     1     0     0    10      3        1    10
## 6     6 placebo        4     1     1     0     6      1        1     6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tail}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 10
##      id treatment number  size recur start  stop status censored times
##   <int> <fct>      <int> <int> <int> <int> <int>  <dbl>    <dbl> <int>
## 1   115 thiotepa       4     1     3    24    47      1        1    23
## 2   115 thiotepa       4     1     3    47    50      0        0     3
## 3   116 thiotepa       3     4     0     0    54      0        0    54
## 4   117 thiotepa       2     1     1     0    38      1        1    38
## 5   117 thiotepa       2     1     1    38    54      0        0    16
## 6   118 thiotepa       1     3     0     0    59      3        1    59
\end{verbatim}

Here the variable ``id'' specifies different individuals, and should be
treated as a random effect. The variable ``time'' is computed using the
difference between variables ``start'' and ``stop'', which denote the
start time and end time of each time interval. It seems like a interval
censoring problem but the start time is known before hand, so we can
treat it as a regular type-I right censoring.

In this study, a nonzero value of ``status'' can be death from bladder
disease, death from other reason or recurrence. Here we will just view
all of these situations as ``occurrence'' for simplicity. So the
variable ``censored'' is created such that it is 1 if ``status'' is
non-zero, otherwise 0. We will include ``number'', ``size'' and
``recur'' as fixed effects in this study.

\hypertarget{survivalcoxph}{%
\subsection{Survival:coxph}\label{survivalcoxph}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bladder.CoxPh <-}\StringTok{ }\KeywordTok{coxph}\NormalTok{(}\KeywordTok{Surv}\NormalTok{(times, censored)}\OperatorTok{~}\StringTok{ }\NormalTok{number }\OperatorTok{+}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{recur }\OperatorTok{+}\StringTok{ }\KeywordTok{frailty.gaussian}\NormalTok{(id), }\DataTypeTok{data =}\NormalTok{ data)}
\KeywordTok{summary}\NormalTok{(bladder.CoxPh)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## coxph(formula = Surv(times, censored) ~ number + size + recur + 
##     frailty.gaussian(id), data = data)
## 
##   n= 294, number of events= 218 
## 
##                      coef     se(coef) se2     Chisq DF   p      
## number               0.064848 0.04246  0.04051  2.33 1.00 1.3e-01
## size                 0.008336 0.04797  0.04631  0.03 1.00 8.6e-01
## recur                0.229848 0.02559  0.02431 80.66 1.00 2.7e-19
## frailty.gaussian(id)                            5.12 4.59 3.5e-01
## 
##        exp(coef) exp(-coef) lower .95 upper .95
## number     1.067     0.9372    0.9818     1.160
## size       1.008     0.9917    0.9179     1.108
## recur      1.258     0.7947    1.1968     1.323
## 
## Iterations: 8 outer, 43 Newton-Raphson
##      Variance of random effect= 0.0257231 
## Degrees of freedom for terms= 0.9 0.9 0.9 4.6 
## Concordance= 0.694  (se = 0.694 )
## Likelihood ratio test= 105.4  on 7.33 df,   p=<2e-16
\end{verbatim}

Fitting this model using the traditional partial likelihood approach
gives insignificant results for all the fixed effects except ``recur'',
which has a strong positive effect. But we will still proceed to check
what will happen if we fit it using a Bayesian approach.

\hypertarget{bayesian-inla-1}{%
\subsection{Bayesian: INLA}\label{bayesian-inla-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula =}\StringTok{ }\KeywordTok{inla.surv}\NormalTok{(times, censored) }\OperatorTok{~}\StringTok{ }\NormalTok{number }\OperatorTok{+}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{recur }\OperatorTok{+}\StringTok{ }\KeywordTok{f}\NormalTok{(id, }\DataTypeTok{model =} \StringTok{"iid"}\NormalTok{)}
\NormalTok{bladder.INLA <-}\StringTok{ }\KeywordTok{inla}\NormalTok{(formula, }\DataTypeTok{control.compute =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{dic =} \OtherTok{TRUE}\NormalTok{), }\DataTypeTok{family =} \StringTok{"coxph"}\NormalTok{, }
                      \DataTypeTok{data =}\NormalTok{ data, }\DataTypeTok{control.hazard=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{model=}\StringTok{"rw2"}\NormalTok{, }\DataTypeTok{n.intervals=}\DecValTok{20}\NormalTok{))}
\NormalTok{bladder.INLA}\OperatorTok{$}\NormalTok{summary.fixed}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                     mean         sd  0.025quant    0.5quant  0.975quant
## (Intercept) -3.945623289 0.26869266 -4.49396667 -3.93815573 -3.43930569
## number       0.061328630 0.04045252 -0.02007881  0.06202240  0.13881958
## size         0.009253826 0.04631398 -0.08484252  0.01037923  0.09704013
## recur        0.215846330 0.02392152  0.16899575  0.21580589  0.26288181
##                    mode          kld
## (Intercept) -3.92340942 5.446235e-06
## number       0.06339940 2.829109e-07
## size         0.01261181 7.225234e-07
## recur        0.21572711 1.743107e-06
\end{verbatim}

Indeed, the two results seem pretty similar in general. In both cases,
we can see that there are no apparent relationships between all of the
fixed effects and the rate of occurrence of bladder cancer's recurrence,
or death, except the variable ``recur'' with a positive effect.


\end{document}
