---
title: "Summary of Simulation Study"
author: "Ziang"
date: "13/11/2019"
output: 
  pdf_document:
    keep_tex: true
    number_sections: true
    fig_caption: yes
header_includes:
  - \newcommand{\ed}{\overset{d}{=}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(parallel)
library(coxed)
library(Matrix)
library(trustOptim)
library(matrixStats) # for LogSumExp
library(purrr)
library(rstanarm)
library(INLA)
# tidyverse packages- required for analysis
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(lubridate)
source("/Users/ziangzhang/Desktop/STA497/current work/code/1. function for PH Model.R")
```

# Simulating Data:

To simulate data from a cox proportional hazard model, we will take use of the Rpackage called "coxed", which is a package that is specifically designed to draw random sample from various kinds of cox models. 

The function that we are going to use is called sim.survdata. The argument "N" specifies the sample size that we are about to generate, and "T" specifies the maximum of times that can be generated. Argument "xvars" specifies the number of covariates in our model, and for simplicity, we set it to three. Argument "censor" specifies the proportion of right censoring in our data, which we set to 0.3.

```{r Simulating1}
set.seed(10086)
N = 200
library(coxed)
simdata <- sim.survdata(N=N, T=200, xvars=3, censor=.3, num.data.frames = 1)
head(simdata$data)
```

The above data-set is generated. All the values of "X1", "X2" and "X3" are generated randomly from three normal distributions. The values of betas are also randomly generated, which can be found as:

```{r TrueBeta}
simdata$betas
```

# Fitting using Coxph:

To begin with, let's fit a cox regression model using frequntist's method "coxph", which give the estimates that maximize the partial likelihood function, together with the corresponding standard errors. To keep it consistent with our algorithm later, we choose to use Breslow's method to deal with ties in the dataset. The results are given at below.

```{r Coxph}
model <- coxph(Surv(y, failed) ~ X1+X2+X3, data=simdata$data,ties = "breslow")
model
```

Based on the output above, it seems like "coxph" does not actually estimate betas very accuately. But it is okay, as our main purpose for now is to compare how similar our algorithm compare to other algorithms. So let's just ignore this inaccuracy and move on to see what other algorithms give for the results.


# Fitting using INLA:

Then, let's fit that dataset using INLA. Different to the "coxph" algorithm above, INLA's algorithm does an approximate Bayesian inference using the full likelihood. Therefore, we should expect that there is some small difference between their results. Furthermore, INLA's algorithm also estimates the baseline hazard using piece-wise constant function, which is ignored in "coxph". 

```{r INLA}
formula <- inla.surv(y,failed) ~ X1+X2+X3
Inlaresult <- inla(formula = formula, control.compute = list(dic=TRUE),data = simdata$data, family = "coxph",
                   control.hazard = list(model="rw1",n.intervals = 20),
                   control.inla = list(strategy = 'gaussian'),
                   control.fixed= list(prec = 0.05))
Inlaresult$summary.fixed
```

Again, the results of INLA are not that close to the true betas of the dataset, but they are close enough to the result of "Coxph". 


# Fitting using Proposed algorithm:

Then, let's use our proposed algorithm to refit this data-set. Currently, our algorithm adjusts the likelihood function for ties using Breslow's method, and later it will have an option to use Efron's method as well. Before that, we need to convert the data into a form that can be used by our functions.

```{r Proposed_DataManipulation}
data <- data_frame(X1=simdata$data$X1,X2=simdata$data$X2,X3=simdata$data$X3, ID = 1:N, censoring = ifelse(simdata$data$failed,1,0), times = simdata$data$y, entry = rep(0,length(simdata$data$y)))
true_etas <- simdata$xb
model_data <- list(n=N,A=NULL,M=0,p=1,Nd = N-1,X=as.matrix(simdata$xdata))
model_data$theta_logprior <- function(theta,prior_alpha = .75,prior_u = log(20)) {
  # In this model, theta is the LOG PRECISION of the rw2 smoothing variance
  # Implement the PC prior directly.
  # P(sigma > u) = alpha.
  # See inla.doc("pc.prec")
  lambda <- -log(prior_alpha)/prior_u
  log(lambda/2) - lambda * exp(-theta/2) - theta/2
}
# log(precision) for prior on beta
model_data$beta_logprec <- log(.05)
model_data$diffmat <- create_diff_matrix(model_data$n)
model_data$lambdainv <- create_full_dtcp_matrix(model_data$n)
model_data$Xd <- model_data$diffmat %*% model_data$X
model_data$p <- ncol(model_data$X)
model_data$Ne <- model_data$Nd + model_data$n
model_data$Wd <- model_data$M + model_data$p + model_data$Nd
model_data$Wdf <- model_data$M + model_data$p + model_data$Ne
model_data$times <- simdata$data$y
model_data$censoring <- data$censoring
model_data$entry <- data$entry
model_data$ID <- data$ID
```

Now, the data is stored in a format that is compatiable with the functions that we wrote, and we can go ahead to refit the model.

```{r Proposed Optimization, include=FALSE}
thetagrid <- list(0) # This is the log(precision)
PARALLEL_EXECUTION <- TRUE
control1 <- list(
  prec = 1e-06,
  stop.trust.radius = 1e-03,
  report.freq = 1,
  report.level = 4,
  start.trust.radius = 100,
  contract.threshold = .25,
  contract.factor = .1,
  expand.factor = 5,
  preconditioner = 1,
  trust.iter = 2000000,
  cg.tol = 1e-06,
  maxit = 1000
)
tm <- proc.time()

sim1opt <- optimize_all_thetas_parallel(
  theta = thetagrid,
  # theta = -6,
  model_data = model_data,
  startingvals = rep(0,model_data$Wd),
  optcontrol = control1,
  doparallel = PARALLEL_EXECUTION
)
rt <- proc.time() - tm
```

The estimated marginal means using our proposed algorithm:
```{r mean}
compute_marginal_means(c(N,N+1,N+2),sim1opt,model_data)
```

The estimated marginal variance using our proposed algorithm:
```{r var}
compute_marginal_variances(c(N,N+1,N+2),sim1opt,model_data)
```

The results are just what we expected. The output given by our algorithm is very close to the outputs given by INLA and Coxph. Furthermore, since our algorithm does the approximate Bayesian inference on Partial likelihood function instead of full likelihood function, the output of our algorithm is much more similar to the output given by Coxph. In general, it seems like our algorithm's result is in the middle of the results of Coxph and INLA, but more lean toward Coxph.


# Fitting using STAN:

Finally, let's redo the inference procedure using STAN. Currently STAN does not have a specific function to fit cox proportional hazard model. Therefore, we need to take advantage of the form of its full-likelihood, and do a Poisson Augmentation trick that enables us to fit the model by setting family = Poisson in STAN.


Firstly, let's do Poisson Augmentation for our dataset, with 20 subintervals:
```{r Poisson}
data_STAN <- data
cut_one <-  max(data_STAN$times) + 1
cut_20 <- c(quantile(data_STAN$times, probs = c(1:19)/20), cut_one) %>% round()
data_STAN_20times <- survival::survSplit(formula = Surv(times, censoring) ~., data = data_STAN, cut = cut_20) %>%
  mutate(interval = factor(entry),
         interval_length = times - entry) %>%
  as_data_frame
head(data_STAN_20times)
```


After the transformation, we can fit the model using STAN:
```{r STAN,include=FALSE}
fit_STAN <- rstanarm::stan_glm(formula = censoring ~ X1+X2+X3 + offset(log(interval_length)), 
                               data = data_STAN_20times, 
                               family = poisson(link = "log"),
                               prior_intercept = normal(location = 0, scale = 4.472136),
                                       prior = normal(location = 0, scale = 4.472136), chains = 100, iter = 10000)
```

The estimated marginal means given by STAN are at below:
```{r STAN2}
fit_STAN$coefficients
```

The estimated marginal standard deviations by STAN:
```{r STAN3}
fit_STAN$ses
```

Finally, let's plot the approximated marginal posteriors (for "coxph", it is the approximated sampling distribution of the estimator) to show the performance of different algorithms for the simulated dataset:
```{r FINAL_Plot,echo=FALSE}
#Our algorithm:
OUR_MEAN <- c(-0.28244408,0.22586592,0.02978612)
OUR_SD <- c(0.1804694,0.1601929,0.1617474)

#INLA's algorithm:
INLA_MEAN <- c(-0.31576677,0.25294302,0.04123724)
INLA_SD <- c(0.1820669,0.1592903,0.1596495)

#Coxph's algorithm: 
COXPH_MEAN <- c(-0.28287174,0.22606094,0.02978013)
COXPH_SD <- c(0.18062,0.16029,0.16186)

#For X1:
hist(as.data.frame(fit_STAN)$X1,freq = F, main = "Approximate marginal posterior for coefficent of X1", xlab = "X1",
     breaks = 1000,col="pink",border = "pink")
lines(dnorm(seq(-1.8,1,by=0.001),mean = OUR_MEAN[1], sd = OUR_SD[1])~seq(-1.8,1,by=0.001),type="l",col="Red")
lines(dnorm(seq(-1.8,1,by=0.001),mean = INLA_MEAN[1], sd =INLA_SD[1])~seq(-1.8,1,by=0.001),type="l",col="Green")
lines(dnorm(seq(-1.8,1,by=0.001),mean = COXPH_MEAN[1], sd =COXPH_SD[1])~seq(-1.8,1,by=0.001),type="l",col="Blue")
legend(-1.8, 2.2, legend=c("Proposed", "INLA", "Coxph(Assume Normal)"),
       col=c("Red", "Green","Blue"), lty=1, cex=0.8)

#For X2:
hist(as.data.frame(fit_STAN)$X2,freq = F, main = "Approximate marginal posterior for coefficent of X2", xlab = "X2",
     breaks = 1000,col="pink",border = "pink")
lines(dnorm(seq(-0.6,1,by=0.001),mean = OUR_MEAN[2], sd = OUR_SD[2])~seq(-0.6,1,by=0.001),type="l",col="Red")
lines(dnorm(seq(-0.6,1,by=0.001),mean = INLA_MEAN[2], sd =INLA_SD[2])~seq(-0.6,1,by=0.001),type="l",col="Green")
lines(dnorm(seq(-0.6,1,by=0.001),mean = COXPH_MEAN[2], sd =COXPH_SD[2])~seq(-0.6,1,by=0.001),type="l",col="Blue")
legend(-0.6, 2.5, legend=c("Proposed", "INLA", "Coxph(Assume Normal)"),
       col=c("Red", "Green","Blue"), lty=1, cex=0.8)



#For X3:
hist(as.data.frame(fit_STAN)$X3,freq = F, main = "Approximate marginal posterior for coefficent of X3", xlab = "X3",
     breaks = 1000,col="pink",border = "pink")
lines(dnorm(seq(-0.8,0.8,by=0.001),mean = OUR_MEAN[3], sd = OUR_SD[3])~seq(-0.8,0.8,by=0.001),type="l",col="Red")
lines(dnorm(seq(-0.8,0.8,by=0.001),mean = INLA_MEAN[3], sd =INLA_SD[3])~seq(-0.8,0.8,by=0.001),type="l",col="Green")
lines(dnorm(seq(-0.8,0.8,by=0.001),mean = COXPH_MEAN[3], sd =COXPH_SD[3])~seq(-0.8,0.8,by=0.001),type="l",col="Blue")
legend(-0.8, 2.5, legend=c("Proposed", "INLA", "Coxph(Assume Normal)"),
       col=c("Red", "Green","Blue"), lty=1, cex=0.8)
```

Based on the plots above, it can be found that in general, our algorithm's output is almost identical to Coxph's output, but slightly different from the output of INLA. The possible explanation is that INLA did everything using the full-likelihood, but our algorithm and Coxph only used the partial likelihood.
One thing that is really worth noticing is that eventhough STAN uses the full-likelihood for the inference, but its result is more similar to Coxph and our proposed algorithm than to INLA, which is not what we expected. 
